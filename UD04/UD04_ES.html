<!DOCTYPE html>

<html class="no-js" lang="es"> <head><meta charset="utf-8"/><meta content="width=device-width,initial-scale=1" name="viewport"/><link href="https://martinezpenya.es/ModelosIA/UD04/UD04_ES.html" rel="canonical"/><link href="../UD03/UD03_ActividadesEntregables.html" rel="prev"/><link href="UD04_ActividadesGuiadas.html" rel="next"/><link href="../assets/favicon.ico" rel="icon"/><meta content="mkdocs-1.6.1, mkdocs-material-9.6.21" name="generator"/><title>An√°lisis de sistemas robotizados - Modelos de IA (CE Inteligencia Artificial y Big Data)</title><link href="../assets/stylesheets/main.2a3383ac.min.css" rel="stylesheet"/><link href="../assets/stylesheets/palette.06af60db.min.css" rel="stylesheet"/><style>:root{--md-admonition-icon--note:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.1%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M64%20480c-35.3%200-64-28.7-64-64V96c0-35.3%2028.7-64%2064-64h320c35.3%200%2064%2028.7%2064%2064v213.5c0%2017-6.7%2033.3-18.7%2045.3L322.7%20461.3c-12%2012-28.3%2018.7-45.3%2018.7zm325.5-176H296c-13.3%200-24%2010.7-24%2024v93.5z%22/%3E%3C/svg%3E');--md-admonition-icon--abstract:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.1%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M384%20512H96c-53%200-96-43-96-96V96C0%2043%2043%200%2096%200h304c26.5%200%2048%2021.5%2048%2048v288c0%2020.9-13.4%2038.7-32%2045.3V448c17.7%200%2032%2014.3%2032%2032s-14.3%2032-32%2032zM96%20384c-17.7%200-32%2014.3-32%2032s14.3%2032%2032%2032h256v-64zm32-232c0%2013.3%2010.7%2024%2024%2024h176c13.3%200%2024-10.7%2024-24s-10.7-24-24-24H152c-13.3%200-24%2010.7-24%2024m24%2072c-13.3%200-24%2010.7-24%2024s10.7%2024%2024%2024h176c13.3%200%2024-10.7%2024-24s-10.7-24-24-24z%22/%3E%3C/svg%3E');--md-admonition-icon--info:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.1%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%20512a256%20256%200%201%200%200-512%20256%20256%200%201%200%200%20512m-32-352a32%2032%200%201%201%2064%200%2032%2032%200%201%201-64%200m-8%2064h48c13.3%200%2024%2010.7%2024%2024v88h8c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-80c-13.3%200-24-10.7-24-24s10.7-24%2024-24h24v-64h-24c-13.3%200-24-10.7-24-24s10.7-24%2024-24%22/%3E%3C/svg%3E');--md-admonition-icon--tip:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.1%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M461.2%2018.9C472.7%2024%20480%2035.4%20480%2048v416c0%2012.6-7.3%2024-18.8%2029.1s-24.8%203.2-34.3-5.1l-46.6-40.7c-43.6-38.1-98.7-60.3-156.4-63V480c0%2017.7-14.3%2032-32%2032h-32c-17.7%200-32-14.3-32-32v-96C57.3%20384%200%20326.7%200%20256s57.3-128%20128-128h84.5c61.8-.2%20121.4-22.7%20167.9-63.3L427%2024c9.4-8.3%2022.9-10.2%2034.3-5.1zM224%20320v.2c70.3%202.7%20137.8%2028.5%20192%2073.4V118.3c-54.2%2044.9-121.7%2070.7-192%2073.4z%22/%3E%3C/svg%3E');--md-admonition-icon--success:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.1%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M434.8%2070.1c14.3%2010.4%2017.5%2030.4%207.1%2044.7l-256%20352c-5.5%207.6-14%2012.3-23.4%2013.1s-18.5-2.7-25.1-9.3l-128-128c-12.5-12.5-12.5-32.8%200-45.3s32.8-12.5%2045.3%200l101.5%20101.5%20234-321.7c10.4-14.3%2030.4-17.5%2044.7-7.1z%22/%3E%3C/svg%3E');--md-admonition-icon--question:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.1%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%20512a256%20256%200%201%200%200-512%20256%20256%200%201%200%200%20512m0-336c-17.7%200-32%2014.3-32%2032%200%2013.3-10.7%2024-24%2024s-24-10.7-24-24c0-44.2%2035.8-80%2080-80s80%2035.8%2080%2080c0%2047.2-36%2067.2-56%2074.5v3.8c0%2013.3-10.7%2024-24%2024s-24-10.7-24-24v-8.1c0-20.5%2014.8-35.2%2030.1-40.2%206.4-2.1%2013.2-5.5%2018.2-10.3%204.3-4.2%207.7-10%207.7-19.6%200-17.7-14.3-32-32-32zm-32%20192a32%2032%200%201%201%2064%200%2032%2032%200%201%201-64%200%22/%3E%3C/svg%3E');--md-admonition-icon--warning:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.1%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M256%200c14.7%200%2028.2%208.1%2035.2%2021l216%20400c6.7%2012.4%206.4%2027.4-.8%2039.5S486.1%20480%20472%20480H40c-14.1%200-27.2-7.4-34.4-19.5s-7.5-27.1-.8-39.5l216-400c7-12.9%2020.5-21%2035.2-21m0%20352a32%2032%200%201%200%200%2064%2032%2032%200%201%200%200-64m0-192c-18.2%200-32.7%2015.5-31.4%2033.7l7.4%20104c.9%2012.5%2011.4%2022.3%2023.9%2022.3%2012.6%200%2023-9.7%2023.9-22.3l7.4-104c1.3-18.2-13.1-33.7-31.4-33.7z%22/%3E%3C/svg%3E');--md-admonition-icon--failure:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20576%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.1%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M480-16c6.9%200%2013%204.4%2015.2%2010.9l13.5%2040.4%2040.4%2013.5C555.6%2051%20560%2057.1%20560%2064s-4.4%2013-10.9%2015.2l-40.4%2013.5-13.5%2040.4c-2.2%206.5-8.3%2010.9-15.2%2010.9s-13-4.4-15.2-10.9l-13.5-40.4-40.4-13.5C404.4%2077%20400%2070.9%20400%2064s4.4-13%2010.9-15.2l40.4-13.5%2013.5-40.4C467-11.6%20473.1-16%20480-16M321.4%2097.4c12.5-12.5%2032.8-12.5%2045.3%200l80%2080c12.5%2012.5%2012.5%2032.8%200%2045.3l-10.9%2010.9c7.9%2022%2012.2%2045.7%2012.2%2070.5%200%20114.9-93.1%20208-208%20208S32%20418.9%2032%20304%20125.1%2096%20240%2096c24.7%200%2048.5%204.3%2070.5%2012.3zM144%20304c0-53%2043-96%2096-96%2013.3%200%2024-10.7%2024-24s-10.7-24-24-24c-79.5%200-144%2064.5-144%20144%200%2013.3%2010.7%2024%2024%2024s24-10.7%2024-24%22/%3E%3C/svg%3E');--md-admonition-icon--danger:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20512%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.1%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M416%20427.4c58.5-44%2096-111.6%2096-187.4C512%20107.5%20397.4%200%20256%200S0%20107.5%200%20240c0%2075.8%2037.5%20143.4%2096%20187.4V464c0%2026.5%2021.5%2048%2048%2048h32v-40c0-13.3%2010.7-24%2024-24s24%2010.7%2024%2024v40h64v-40c0-13.3%2010.7-24%2024-24s24%2010.7%2024%2024v40h32c26.5%200%2048-21.5%2048-48zM96%20256a64%2064%200%201%201%20128%200%2064%2064%200%201%201-128%200m256-64a64%2064%200%201%201%200%20128%2064%2064%200%201%201%200-128%22/%3E%3C/svg%3E');--md-admonition-icon--bug:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20640%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.1%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M352%200c0-17.7-14.3-32-32-32s-32%2014.3-32%2032v64h-96c-53%200-96%2043-96%2096v224c0%2053%2043%2096%2096%2096h256c53%200%2096-43%2096-96V160c0-53-43-96-96-96h-96zM160%20368c0-13.3%2010.7-24%2024-24h32c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-32c-13.3%200-24-10.7-24-24m120%200c0-13.3%2010.7-24%2024-24h32c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-32c-13.3%200-24-10.7-24-24m120%200c0-13.3%2010.7-24%2024-24h32c13.3%200%2024%2010.7%2024%2024s-10.7%2024-24%2024h-32c-13.3%200-24-10.7-24-24M224%20176a48%2048%200%201%201%200%2096%2048%2048%200%201%201%200-96m144%2048a48%2048%200%201%201%2096%200%2048%2048%200%201%201-96%200m-304%200c0-17.7-14.3-32-32-32S0%20206.3%200%20224v96c0%2017.7%2014.3%2032%2032%2032s32-14.3%2032-32zm544-32c-17.7%200-32%2014.3-32%2032v96c0%2017.7%2014.3%2032%2032%2032s32-14.3%2032-32v-96c0-17.7-14.3-32-32-32%22/%3E%3C/svg%3E');--md-admonition-icon--example:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.1%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M248%20106.6c18.9-9%2032-28.3%2032-50.6%200-30.9-25.1-56-56-56s-56%2025.1-56%2056c0%2022.3%2013.1%2041.6%2032%2050.6v98.8c-2.8%201.3-5.5%202.9-8%204.7l-80.1-45.8c1.6-20.8-8.6-41.6-27.9-52.8C57.2%2096%2023%20105.2%207.5%20132S1.2%20193%2028%20208.5c1.3.8%202.6%201.5%204%202.1v90.8c-1.3.6-2.7%201.3-4%202.1C1.2%20319-8%20353.2%207.5%20380s49.7%2036%2076.5%2020.5c19.3-11.1%2029.4-32%2027.8-52.8l50.5-28.9c-11.5-11.2-19.9-25.6-23.8-41.7l-50.5%2029c-2.6-1.8-5.2-3.3-8-4.7v-90.8c2.8-1.3%205.5-2.9%208-4.7l80.1%2045.8c-.1%201.4-.2%202.8-.2%204.3%200%2022.3%2013.1%2041.6%2032%2050.6v98.8c-18.9%209-32%2028.3-32%2050.6%200%2030.9%2025.1%2056%2056%2056s56-25.1%2056-56c0-22.3-13.1-41.6-32-50.6v-98.8c2.8-1.3%205.5-2.9%208-4.7l80.1%2045.8c-1.6%2020.8%208.6%2041.6%2027.8%2052.8%2026.8%2015.5%2061%206.3%2076.5-20.5s6.3-61-20.5-76.5c-1.3-.8-2.7-1.5-4-2.1v-90.8c1.4-.6%202.7-1.3%204-2.1%2026.8-15.5%2036-49.7%2020.5-76.5s-49.5-36-76.3-20.5c-19.3%2011.1-29.4%2032-27.8%2052.8l-50.6%2028.9c11.5%2011.2%2019.9%2025.6%2023.8%2041.7l50.6-29c2.6%201.8%205.2%203.3%208%204.7v90.8c-2.8%201.3-5.5%202.9-8%204.6l-80.1-45.8c.1-1.4.2-2.8.2-4.3%200-22.3-13.1-41.6-32-50.6v-98.8z%22/%3E%3C/svg%3E');--md-admonition-icon--quote:url('data:image/svg+xml;charset=utf-8,%3Csvg%20xmlns%3D%22http%3A//www.w3.org/2000/svg%22%20viewBox%3D%220%200%20448%20512%22%3E%3C%21--%21%20Font%20Awesome%20Free%207.0.1%20by%20%40fontawesome%20-%20https%3A//fontawesome.com%20License%20-%20https%3A//fontawesome.com/license/free%20%28Icons%3A%20CC%20BY%204.0%2C%20Fonts%3A%20SIL%20OFL%201.1%2C%20Code%3A%20MIT%20License%29%20Copyright%202025%20Fonticons%2C%20Inc.--%3E%3Cpath%20d%3D%22M0%20216C0%20149.7%2053.7%2096%20120%2096h8c17.7%200%2032%2014.3%2032%2032s-14.3%2032-32%2032h-8c-30.9%200-56%2025.1-56%2056v8h64c35.3%200%2064%2028.7%2064%2064v64c0%2035.3-28.7%2064-64%2064H64c-35.3%200-64-28.7-64-64zm256%200c0-66.3%2053.7-120%20120-120h8c17.7%200%2032%2014.3%2032%2032s-14.3%2032-32%2032h-8c-30.9%200-56%2025.1-56%2056v8h64c35.3%200%2064%2028.7%2064%2064v64c0%2035.3-28.7%2064-64%2064h-64c-35.3%200-64-28.7-64-64z%22/%3E%3C/svg%3E');}</style><script src="https://unpkg.com/iframe-worker/shim"></script><link crossorigin="" href="https://fonts.gstatic.com" rel="preconnect"/><link href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&amp;display=fallback" rel="stylesheet"/><style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style><link href="../css/extra.css" rel="stylesheet"/><script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><link href="../assets/stylesheets/extra-style.svsxn5ox.min.css" rel="stylesheet"/></head> <body data-md-color-accent="lime" data-md-color-primary="teal" data-md-color-scheme="default" dir="ltr"> <input autocomplete="off" class="md-toggle" data-md-toggle="drawer" id="__drawer" type="checkbox"/> <input autocomplete="off" class="md-toggle" data-md-toggle="search" id="__search" type="checkbox"/> <label class="md-overlay" for="__drawer"></label> <div data-md-component="skip"> <a class="md-skip" href="#robots"> Saltar a contenido </a> </div> <div data-md-component="announce"> </div> <header class="md-header md-header--shadow" data-md-component="header"> <nav aria-label="Cabecera" class="md-header__inner md-grid"> <a aria-label="Modelos de IA (CE Inteligencia Artificial y Big Data)" class="md-header__button md-logo" data-md-component="logo" href=".." title="Modelos de IA (CE Inteligencia Artificial y Big Data)"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 8h1v1h-1zm1-3h-1v2h1V6h.5c.28 0 .5-.22.5-.5v-2c0-.28-.22-.5-.5-.5H18v1h2zm-3-2h-1v4h1zm-3.5 12.5a2 2 0 1 0 4 0c0-1.11-.89-2-2-2s-2 .9-2 2M17 8h-1v1h1zm5 6h-1c0-1.5-.47-2.87-1.26-4h-2.77c1.22.91 2.03 2.36 2.03 4v2h2v1h-2v3H5v-3H3v-1h2v-2c0-2.76 2.24-5 5-5h4c.34 0 .68.04 1 .1V7.08c-.33-.05-.66-.08-1-.08h-1V5.73A2 2 0 1 0 10 4c0 .74.4 1.39 1 1.73V7h-1c-3.87 0-7 3.13-7 7H2c-.55 0-1 .45-1 1v3c0 .55.45 1 1 1h1v1a2 2 0 0 0 2 2h14c1.11 0 2-.89 2-2v-1h1c.55 0 1-.45 1-1v-3c0-.55-.45-1-1-1m-13.5-.5c-1.1 0-2 .9-2 2s.9 2 2 2 2-.89 2-2-.89-2-2-2"></path></svg> </a> <label class="md-header__button md-icon" for="__drawer"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class="md-header__title" data-md-component="header-title"> <div class="md-header__ellipsis"> <div class="md-header__topic"> <span class="md-ellipsis"> Modelos de IA (CE Inteligencia Artificial y Big Data) </span> </div> <div class="md-header__topic" data-md-component="header-topic"> <span class="md-ellipsis"> An√°lisis de sistemas robotizados </span> </div> </div> </div> <form class="md-header__option" data-md-component="palette"> <input aria-label="Light mode" class="md-option" data-md-color-accent="lime" data-md-color-media="" data-md-color-primary="teal" data-md-color-scheme="default" id="__palette_0" name="__palette" type="radio"/> <label class="md-header__button md-icon" for="__palette_1" hidden="" title="Light mode"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="m17.75 4.09-2.53 1.94.91 3.06-2.63-1.81-2.63 1.81.91-3.06-2.53-1.94L12.44 4l1.06-3 1.06 3zm3.5 6.91-1.64 1.25.59 1.98-1.7-1.17-1.7 1.17.59-1.98L15.75 11l2.06-.05L18.5 9l.69 1.95zm-2.28 4.95c.83-.08 1.72 1.1 1.19 1.85-.32.45-.66.87-1.08 1.27C15.17 23 8.84 23 4.94 19.07c-3.91-3.9-3.91-10.24 0-14.14.4-.4.82-.76 1.27-1.08.75-.53 1.93.36 1.85 1.19-.27 2.86.69 5.83 2.89 8.02a9.96 9.96 0 0 0 8.02 2.89m-1.64 2.02a12.08 12.08 0 0 1-7.8-3.47c-2.17-2.19-3.33-5-3.49-7.82-2.81 3.14-2.7 7.96.31 10.98 3.02 3.01 7.84 3.12 10.98.31"></path></svg> </label> <input aria-label="Dark mode" class="md-option" data-md-color-accent="lime" data-md-color-media="" data-md-color-primary="teal" data-md-color-scheme="slate" id="__palette_1" name="__palette" type="radio"/> <label class="md-header__button md-icon" for="__palette_0" hidden="" title="Dark mode"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M12 7a5 5 0 0 1 5 5 5 5 0 0 1-5 5 5 5 0 0 1-5-5 5 5 0 0 1 5-5m0 2a3 3 0 0 0-3 3 3 3 0 0 0 3 3 3 3 0 0 0 3-3 3 3 0 0 0-3-3m0-7 2.39 3.42C13.65 5.15 12.84 5 12 5s-1.65.15-2.39.42zM3.34 7l4.16-.35A7.2 7.2 0 0 0 5.94 8.5c-.44.74-.69 1.5-.83 2.29zm.02 10 1.76-3.77a7.131 7.131 0 0 0 2.38 4.14zM20.65 7l-1.77 3.79a7.02 7.02 0 0 0-2.38-4.15zm-.01 10-4.14.36c.59-.51 1.12-1.14 1.54-1.86.42-.73.69-1.5.83-2.29zM12 22l-2.41-3.44c.74.27 1.55.44 2.41.44.82 0 1.63-.17 2.37-.44z"></path></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for="__search"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class="md-search" data-md-component="search" role="dialog"> <label class="md-search__overlay" for="__search"></label> <div class="md-search__inner" role="search"> <form class="md-search__form" name="search"> <input aria-label="B√∫squeda" autocapitalize="off" autocomplete="off" autocorrect="off" class="md-search__input" data-md-component="search-query" name="query" placeholder="B√∫squeda" required="" spellcheck="false" type="text"/> <label class="md-search__icon md-icon" for="__search"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav aria-label="Buscar" class="md-search__options"> <button aria-label="Limpiar" class="md-search__icon md-icon" tabindex="-1" title="Limpiar" type="reset"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> </form> <div class="md-search__output"> <div class="md-search__scrollwrap" data-md-scrollfix="" tabindex="0"> <div class="md-search-result" data-md-component="search-result"> <div class="md-search-result__meta"> Inicializando b√∫squeda </div> <ol class="md-search-result__list" role="presentation"></ol> </div> </div> </div> </div> </div> <div class="md-header__source"> <a class="md-source" data-md-component="source" href="https://github.com/martinezpenya/ModelosIA" title="Ir al repositorio"> <div class="md-source__icon md-icon"> <svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg> </div> <div class="md-source__repository"> GitHub </div> </a> </div> <a class="md-header-nav__button md-icon" href="../docs/Libro.pdf" title="PDF">
<svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg">
<path d="M128,0c-17.6,0-32,14.4-32,32v448c0,17.6,14.4,32,32,32h320c17.6,0,32-14.4,32-32V128L352,0H128z" fill="#E2E5E7"></path>
<path d="m384 128h96l-128-128v96c0 17.6 14.4 32 32 32z" fill="#B0B7BD"></path>
<polygon fill="#CAD1D8" points="480 224 384 128 480 128"></polygon>
<path d="M416,416c0,8.8-7.2,16-16,16H48c-8.8,0-16-7.2-16-16V256c0-8.8,7.2-16,16-16h352c8.8,0,16,7.2,16,16  V416z" fill="#F15642"></path>
<g fill="#fff">
<path d="m101.74 303.15c0-4.224 3.328-8.832 8.688-8.832h29.552c16.64 0 31.616 11.136 31.616 32.48 0 20.224-14.976 31.488-31.616 31.488h-21.36v16.896c0 5.632-3.584 8.816-8.192 8.816-4.224 0-8.688-3.184-8.688-8.816v-72.032zm16.88 7.28v31.872h21.36c8.576 0 15.36-7.568 15.36-15.504 0-8.944-6.784-16.368-15.36-16.368h-21.36z"></path>
<path d="m196.66 384c-4.224 0-8.832-2.304-8.832-7.92v-72.672c0-4.592 4.608-7.936 8.832-7.936h29.296c58.464 0 57.184 88.528 1.152 88.528h-30.448zm8.064-72.912v57.312h21.232c34.544 0 36.08-57.312 0-57.312h-21.232z"></path>
<path d="m303.87 312.11v20.336h32.624c4.608 0 9.216 4.608 9.216 9.072 0 4.224-4.608 7.68-9.216 7.68h-32.624v26.864c0 4.48-3.184 7.92-7.664 7.92-5.632 0-9.072-3.44-9.072-7.92v-72.672c0-4.592 3.456-7.936 9.072-7.936h44.912c5.632 0 8.96 3.344 8.96 7.936 0 4.096-3.328 8.704-8.96 8.704h-37.248v0.016z"></path>
</g>
<path d="m400 432h-304v16h304c8.8 0 16-7.2 16-16v-16c0 8.8-7.2 16-16 16z" fill="#CAD1D8"></path>
</svg>
</a></nav> </header> <div class="md-container" data-md-component="container"> <main class="md-main" data-md-component="main"> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation"> <div class="md-sidebar__scrollwrap"> <div class="md-sidebar__inner"> <nav aria-label="Navegaci√≥n" class="md-nav md-nav--primary md-nav--integrated" data-md-level="0"> <label class="md-nav__title" for="__drawer"> <a aria-label="Modelos de IA (CE Inteligencia Artificial y Big Data)" class="md-nav__button md-logo" data-md-component="logo" href=".." title="Modelos de IA (CE Inteligencia Artificial y Big Data)"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M19 8h1v1h-1zm1-3h-1v2h1V6h.5c.28 0 .5-.22.5-.5v-2c0-.28-.22-.5-.5-.5H18v1h2zm-3-2h-1v4h1zm-3.5 12.5a2 2 0 1 0 4 0c0-1.11-.89-2-2-2s-2 .9-2 2M17 8h-1v1h1zm5 6h-1c0-1.5-.47-2.87-1.26-4h-2.77c1.22.91 2.03 2.36 2.03 4v2h2v1h-2v3H5v-3H3v-1h2v-2c0-2.76 2.24-5 5-5h4c.34 0 .68.04 1 .1V7.08c-.33-.05-.66-.08-1-.08h-1V5.73A2 2 0 1 0 10 4c0 .74.4 1.39 1 1.73V7h-1c-3.87 0-7 3.13-7 7H2c-.55 0-1 .45-1 1v3c0 .55.45 1 1 1h1v1a2 2 0 0 0 2 2h14c1.11 0 2-.89 2-2v-1h1c.55 0 1-.45 1-1v-3c0-.55-.45-1-1-1m-13.5-.5c-1.1 0-2 .9-2 2s.9 2 2 2 2-.89 2-2-.89-2-2-2"></path></svg> </a> Modelos de IA (CE Inteligencia Artificial y Big Data) </label> <div class="md-nav__source"> <a class="md-source" data-md-component="source" href="https://github.com/martinezpenya/ModelosIA" title="Ir al repositorio"> <div class="md-source__icon md-icon"> <svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg> </div> <div class="md-source__repository"> GitHub </div> </a> </div> <ul class="md-nav__list" data-md-scrollfix=""> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id="__nav_1" type="checkbox"/> <label class="md-nav__link" for="__nav_1" id="__nav_1_label" tabindex="0"> <span class="md-ellipsis"> UD00 </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded="false" aria-labelledby="__nav_1_label" class="md-nav" data-md-level="1"> <label class="md-nav__title" for="__nav_1"> <span class="md-nav__icon md-icon"></span> UD00 </label> <ul class="md-nav__list" data-md-scrollfix=""> <li class="md-nav__item"> <a class="md-nav__link" href="../index.html"> <span class="md-ellipsis"> Informaci√≥n importante </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id="__nav_2" type="checkbox"/> <label class="md-nav__link" for="__nav_2" id="__nav_2_label" tabindex="0"> <span class="md-ellipsis"> Docker </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded="false" aria-labelledby="__nav_2_label" class="md-nav" data-md-level="1"> <label class="md-nav__title" for="__nav_2"> <span class="md-nav__icon md-icon"></span> Docker </label> <ul class="md-nav__list" data-md-scrollfix=""> <li class="md-nav__item"> <a class="md-nav__link" href="../Docker/Docker.html"> <span class="md-ellipsis"> Introducci√≥n a docker </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id="__nav_3" type="checkbox"/> <label class="md-nav__link" for="__nav_3" id="__nav_3_label" tabindex="0"> <span class="md-ellipsis"> UD01 </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded="false" aria-labelledby="__nav_3_label" class="md-nav" data-md-level="1"> <label class="md-nav__title" for="__nav_3"> <span class="md-nav__icon md-icon"></span> UD01 </label> <ul class="md-nav__list" data-md-scrollfix=""> <li class="md-nav__item"> <a class="md-nav__link" href="../UD01/UD01_ES.html"> <span class="md-ellipsis"> Caracterizaci√≥n de sistemas y utilizaci√≥n de modelos de Inteligencia Artificial </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../UD01/diapositivas.html"> <span class="md-ellipsis"> Diapositivas </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id="__nav_3_3" type="checkbox"/> <label class="md-nav__link" for="__nav_3_3" id="__nav_3_3_label" tabindex="0"> <span class="md-ellipsis"> Actividades </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded="false" aria-labelledby="__nav_3_3_label" class="md-nav" data-md-level="2"> <label class="md-nav__title" for="__nav_3_3"> <span class="md-nav__icon md-icon"></span> Actividades </label> <ul class="md-nav__list" data-md-scrollfix=""> <li class="md-nav__item"> <a class="md-nav__link" href="../UD01/UD01_P01_RobocodeTankRoyale_ES.html"> <span class="md-ellipsis"> P01 Robocode </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../UD01/UD01_P02_Practica_ES.html"> <span class="md-ellipsis"> P02 Entregable </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id="__nav_3_4" type="checkbox"/> <label class="md-nav__link" for="__nav_3_4" id="__nav_3_4_label" tabindex="0"> <span class="md-ellipsis"> Talleres </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded="false" aria-labelledby="__nav_3_4_label" class="md-nav" data-md-level="2"> <label class="md-nav__title" for="__nav_3_4"> <span class="md-nav__icon md-icon"></span> Talleres </label> <ul class="md-nav__list" data-md-scrollfix=""> <li class="md-nav__item"> <a class="md-nav__link" href="../UD01/UD01_T01_IDE_ES.html"> <span class="md-ellipsis"> T01 Java </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../UD01/UD01_T02_GitHub.html"> <span class="md-ellipsis"> T02 GitHub </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../UD01/UD01_T03_Markdown.html"> <span class="md-ellipsis"> T03 Markdown </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id="__nav_4" type="checkbox"/> <label class="md-nav__link" for="__nav_4" id="__nav_4_label" tabindex="0"> <span class="md-ellipsis"> UD02 </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded="false" aria-labelledby="__nav_4_label" class="md-nav" data-md-level="1"> <label class="md-nav__title" for="__nav_4"> <span class="md-nav__icon md-icon"></span> UD02 </label> <ul class="md-nav__list" data-md-scrollfix=""> <li class="md-nav__item"> <a class="md-nav__link" href="../UD02/UD02_ES.html"> <span class="md-ellipsis"> Sistemas Expertos </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../UD02/UD02_Diapositivas.html"> <span class="md-ellipsis"> Diapositivas </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../UD02/UD02_ActividadesGuiadas.html"> <span class="md-ellipsis"> Actividades Guiadas </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../UD02/UD02_ActividadesEntregables.html"> <span class="md-ellipsis"> Actividades Entregables </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id="__nav_5" type="checkbox"/> <label class="md-nav__link" for="__nav_5" id="__nav_5_label" tabindex="0"> <span class="md-ellipsis"> UD03 </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded="false" aria-labelledby="__nav_5_label" class="md-nav" data-md-level="1"> <label class="md-nav__title" for="__nav_5"> <span class="md-nav__icon md-icon"></span> UD03 </label> <ul class="md-nav__list" data-md-scrollfix=""> <li class="md-nav__item"> <a class="md-nav__link" href="../UD03/UD03_ES.html"> <span class="md-ellipsis"> Procesamiento del Lenguaje Natural </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../UD03/UD03_Diapositivas.html"> <span class="md-ellipsis"> Diapositivas </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../UD03/UD03_ActividadesGuiadas.html"> <span class="md-ellipsis"> Actividades Guiadas </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../UD03/UD03_ActividadesEntregables.html"> <span class="md-ellipsis"> Actividades Entregables </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input checked="" class="md-nav__toggle md-toggle" id="__nav_6" type="checkbox"/> <label class="md-nav__link" for="__nav_6" id="__nav_6_label" tabindex="0"> <span class="md-ellipsis"> UD04 </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded="true" aria-labelledby="__nav_6_label" class="md-nav" data-md-level="1"> <label class="md-nav__title" for="__nav_6"> <span class="md-nav__icon md-icon"></span> UD04 </label> <ul class="md-nav__list" data-md-scrollfix=""> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" id="__toc" type="checkbox"/> <label class="md-nav__link md-nav__link--active" for="__toc"> <span class="md-ellipsis"> An√°lisis de sistemas robotizados </span> <span class="md-nav__icon md-icon"></span> </label> <a class="md-nav__link md-nav__link--active" href="UD04_ES.html"> <span class="md-ellipsis"> An√°lisis de sistemas robotizados </span> </a> <nav aria-label="Tabla de contenidos" class="md-nav md-nav--secondary"> <label class="md-nav__title" for="__toc"> <span class="md-nav__icon md-icon"></span> Tabla de contenidos </label> <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix=""> <li class="md-nav__item"> <a class="md-nav__link" href="#robots"> <span class="md-ellipsis"> Robots </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#hardware-de-robots"> <span class="md-ellipsis"> Hardware de Robots </span> </a> <nav aria-label="Hardware de Robots" class="md-nav"> <ul class="md-nav__list"> <li class="md-nav__item"> <a class="md-nav__link" href="#tipos-de-robots-desde-la-perspectiva-del-hardware"> <span class="md-ellipsis"> Tipos de robots desde la perspectiva del hardware </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#sintiendo-el-mundo"> <span class="md-ellipsis"> Sintiendo el mundo </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#produciendo-movimiento"> <span class="md-ellipsis"> Produciendo movimiento </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#que-tipo-de-problema-resuelve-la-robotica"> <span class="md-ellipsis"> ¬øQu√© tipo de problema resuelve la rob√≥tica? </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#percepcion-robotica"> <span class="md-ellipsis"> Percepci√≥n rob√≥tica </span> </a> <nav aria-label="Percepci√≥n rob√≥tica" class="md-nav"> <ul class="md-nav__list"> <li class="md-nav__item"> <a class="md-nav__link" href="#localizacion-y-mapeo"> <span class="md-ellipsis"> Localizaci√≥n y mapeo </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#otros-tipos-de-percepcion"> <span class="md-ellipsis"> Otros tipos de percepci√≥n </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#aprendizaje-supervisado-y-no-supervisado-en-percepcion-de-robots"> <span class="md-ellipsis"> Aprendizaje supervisado y no supervisado en percepci√≥n de robots </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#planificacion-y-control"> <span class="md-ellipsis"> Planificaci√≥n y Control </span> </a> <nav aria-label="Planificaci√≥n y Control" class="md-nav"> <ul class="md-nav__list"> <li class="md-nav__item"> <a class="md-nav__link" href="#espacio-de-configuracion"> <span class="md-ellipsis"> Espacio de configuraci√≥n </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#planificacion-de-movimiento"> <span class="md-ellipsis"> Planificaci√≥n de movimiento </span> </a> <nav aria-label="Planificaci√≥n de movimiento" class="md-nav"> <ul class="md-nav__list"> <li class="md-nav__item"> <a class="md-nav__link" href="#graficos-de-visibilidad"> <span class="md-ellipsis"> Gr√°ficos de visibilidad </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#diagramas-de-voronoi"> <span class="md-ellipsis"> Diagramas de Voronoi </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#descomposicion-celular"> <span class="md-ellipsis"> Descomposici√≥n celular </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#planificacion-de-movimiento-aleatoria-probabilistic-roadmap-prm"> <span class="md-ellipsis"> Planificaci√≥n de movimiento aleatoria (Probabilistic RoadMap PRM) </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#arboles-aleatorios-que-se-exploran-rapidamente"> <span class="md-ellipsis"> √Årboles aleatorios que se exploran r√°pidamente </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#optimizacion-de-trayectoria-para-planificacion-cinematica"> <span class="md-ellipsis"> Optimizaci√≥n de trayectoria para planificaci√≥n cinem√°tica </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#control-de-seguimiento-de-trayectoria"> <span class="md-ellipsis"> Control de seguimiento de trayectoria </span> </a> <nav aria-label="Control de seguimiento de trayectoria" class="md-nav"> <ul class="md-nav__list"> <li class="md-nav__item"> <a class="md-nav__link" href="#planes-versus-politicas"> <span class="md-ellipsis"> Planes versus pol√≠ticas </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#control-optimo"> <span class="md-ellipsis"> Control √≥ptimo </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#planificacion-de-movimientos-inciertos"> <span class="md-ellipsis"> Planificaci√≥n de movimientos inciertos </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#aprendizaje-por-refuerzo-en-robotica"> <span class="md-ellipsis"> Aprendizaje por refuerzo en rob√≥tica </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#humanos-y-robots"> <span class="md-ellipsis"> Humanos y robots </span> </a> <nav aria-label="Humanos y robots" class="md-nav"> <ul class="md-nav__list"> <li class="md-nav__item"> <a class="md-nav__link" href="#coordinacion"> <span class="md-ellipsis"> Coordinaci√≥n </span> </a> <nav aria-label="Coordinaci√≥n" class="md-nav"> <ul class="md-nav__list"> <li class="md-nav__item"> <a class="md-nav__link" href="#los-humanos-como-agentes-aproximadamente-racionales"> <span class="md-ellipsis"> Los humanos como agentes aproximadamente racionales </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#predecir-la-accion-humana"> <span class="md-ellipsis"> Predecir la acci√≥n humana </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#predicciones-humanas-sobre-el-robot"> <span class="md-ellipsis"> Predicciones humanas sobre el robot. </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#aprender-a-hacer-lo-que-los-humanos-quieren"> <span class="md-ellipsis"> Aprender a hacer lo que los humanos quieren </span> </a> <nav aria-label="Aprender a hacer lo que los humanos quieren" class="md-nav"> <ul class="md-nav__list"> <li class="md-nav__item"> <a class="md-nav__link" href="#aprendizaje-preferencial-funciones-de-costo-de-aprendizaje"> <span class="md-ellipsis"> Aprendizaje preferencial: funciones de costo de aprendizaje </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#politicas-de-aprendizaje-directamente-a-traves-de-la-imitacion"> <span class="md-ellipsis"> Pol√≠ticas de aprendizaje directamente a trav√©s de la imitaci√≥n </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#dominios-de-aplicacion"> <span class="md-ellipsis"> Dominios de aplicaci√≥n </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="#resumen"> <span class="md-ellipsis"> Resumen </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a class="md-nav__link" href="UD04_ActividadesGuiadas.html"> <span class="md-ellipsis"> Actividades Guiadas </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="UD04_ActividadesEntregables.html"> <span class="md-ellipsis"> Actividades Entregables </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle" id="__nav_7" type="checkbox"/> <label class="md-nav__link" for="__nav_7" id="__nav_7_label" tabindex="0"> <span class="md-ellipsis"> UD05 </span> <span class="md-nav__icon md-icon"></span> </label> <nav aria-expanded="false" aria-labelledby="__nav_7_label" class="md-nav" data-md-level="1"> <label class="md-nav__title" for="__nav_7"> <span class="md-nav__icon md-icon"></span> UD05 </label> <ul class="md-nav__list" data-md-scrollfix=""> <li class="md-nav__item"> <a class="md-nav__link" href="../UD05/UD05_ES.html"> <span class="md-ellipsis"> Aplicaci√≥n de principios legales y √©ticos de la IA </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../UD05/UD05_ActividadesGuiadas.html"> <span class="md-ellipsis"> Actividades Guiadas </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../UD05/UD05_ActividadesEntregables.html"> <span class="md-ellipsis"> Actividades Entregables </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../sobremi.html"> <span class="md-ellipsis"> Sobre mi </span> </a> </li> <li class="md-nav__item"> <a class="md-nav__link" href="../fuentes.html"> <span class="md-ellipsis"> Fuentes de informaci√≥n </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-content" data-md-component="content"> <article class="md-content__inner md-typeset"> <a class="md-content__button md-icon" href="https://github.com/martinezpenya/ModelosIA/edit/main/docs/UD04/UD04_ES.md" rel="edit" title="Editar esta p√°gina"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M10 20H6V4h7v5h5v3.1l2-2V8l-6-6H6c-1.1 0-2 .9-2 2v16c0 1.1.9 2 2 2h4zm10.2-7c.1 0 .3.1.4.2l1.3 1.3c.2.2.2.6 0 .8l-1 1-2.1-2.1 1-1c.1-.1.2-.2.4-.2m0 3.9L14.1 23H12v-2.1l6.1-6.1z"></path></svg> </a> <h1>An√°lisis de sistemas robotizados</h1> <div><p><iframe allow="autoplay; fullscreen; picture-in-picture" allowfullscreen="" frameborder="0" height="360" src="https://player.vimeo.com/video/639236696?h=740f412ce5" width="100%"></iframe></p> <h2 id="robots">Robots<a class="headerlink" href="#robots" title="Permanent link">¬∂</a></h2> <p>Los robots son agentes f√≠sicos que realizan tareas manipulando el mundo f√≠sico. Para ello, est√°n equipados con efectores como patas, ruedas, articulaciones y pinzas. Los efectores est√°n dise√±ados para ejercer fuerzas f√≠sicas sobre el medio ambiente. Cuando hacen esto, pueden suceder algunas cosas: el estado del robot puede cambiar (por ejemplo, un autom√≥vil hace girar sus ruedas y, como resultado, avanza en la carretera), el estado del entorno puede cambiar (por ejemplo, un brazo rob√≥tico usa su agarre para empujar una taza a trav√©s del mostrador), e incluso el estado de las personas alrededor del robot podr√≠a cambiar (por ejemplo, un exoesqueleto se mueve y eso cambia la configuraci√≥n de la pierna de una persona; o un robot m√≥vil avanza hacia las puertas del ascensor, y una persona se da cuenta y es lo suficientemente amable como para apartarse del camino o incluso presionar el bot√≥n del robot).</p> <p>Los robots tambi√©n est√°n equipados con sensores que les permiten percibir su entorno. La rob√≥tica actual emplea un conjunto diverso de sensores, que incluyen c√°maras, radares, l√°seres y micr√≥fonos para medir el estado del medio ambiente y de las personas que lo rodean; y giroscopios, sensores de tensi√≥n y torsi√≥n, y aceler√≥metros para medir el propio estado del robot.</p> <p>Maximizar la utilidad esperada de un robot significa elegir c√≥mo activar sus efectores para hacer valer las fuerzas f√≠sicas correctas, aquellas que conducir√°n a cambios de estado que acumular√°n la mayor recompensa esperada posible. En √∫ltima instancia, los robots intentan realizar alguna tarea en el mundo f√≠sico.</p> <p>Los robots operan en entornos que son parcialmente observables y <a href="https://es.wikipedia.org/wiki/Estoc%C3%A1stico">estoc√°sticos</a>: las c√°maras no pueden ver en las esquinas y los engranajes pueden patinar. Adem√°s, las personas que act√∫an en ese mismo entorno son impredecibles, por lo que el robot necesita hacer predicciones sobre ellas.</p> <p>Los robots suelen modelar su entorno con un espacio de estado continuo (la posici√≥n del robot tiene coordenadas continuas) y un espacio de acci√≥n continuo (la cantidad de corriente que un robot env√≠a a su motor tambi√©n se mide en unidades continuas). Algunos robots operan en espacios de grandes dimensiones: los coches necesitan conocer la posici√≥n, orientaci√≥n y velocidad de ellos mismos y de los agentes cercanos; los brazos del robot tienen seis o siete articulaciones y cada una se puede mover de forma independiente; y los robots que imitan el cuerpo humano tienen cientos de articulaciones.</p> <p>El aprendizaje rob√≥tica est√° limitado porque el mundo real se niega obstinadamente a operar m√°s r√°pido que el tiempo real. En un entorno simulado, es posible utilizar algoritmos de aprendizaje (como el <a href="https://es.wikipedia.org/wiki/Q-learning">algoritmo Q-learning</a>) para aprender en unas pocas horas a partir de millones de pruebas. En un entorno real, llevar a cabo estas pruebas podr√≠a llevar a√±os y el robot no puede arriesgarse (y por lo tanto no puede aprender) a una prueba que podr√≠a causar da√±o. Por lo tanto, transferir lo aprendido en la simulaci√≥n a un robot real en el mundo real (el problema de simulaci√≥n a real) es un √°rea activa de investigaci√≥n. Los sistemas rob√≥ticos pr√°cticos deben incorporar conocimientos previos sobre el robot, el entorno f√≠sico y las tareas a realizar para que el robot pueda aprender r√°pidamente y desempe√±arse de forma segura.</p> <p>La rob√≥tica re√∫ne muchos de los conceptos, incluida la estimaci√≥n probabil√≠stica del estado, la percepci√≥n, la planificaci√≥n, el aprendizaje no supervisado, el aprendizaje por refuerzo y la teor√≠a de juegos. Para algunos de estos conceptos, la rob√≥tica sirve como un ejemplo de aplicaci√≥n desafiante.</p> <h2 id="hardware-de-robots">Hardware de Robots<a class="headerlink" href="#hardware-de-robots" title="Permanent link">¬∂</a></h2> <p>Hasta ahora nos hemos concentrado en el programa del agente. Pero el √©xito de los robots reales depende al menos en la misma medida del dise√±o de sensores y efectores que sean apropiados para la tarea.</p> <h3 id="tipos-de-robots-desde-la-perspectiva-del-hardware">Tipos de robots desde la perspectiva del hardware<a class="headerlink" href="#tipos-de-robots-desde-la-perspectiva-del-hardware" title="Permanent link">¬∂</a></h3> <p>Cuando piensas en un robot, puedes imaginar algo con una cabeza y dos brazos, movi√©ndose sobre piernas o ruedas. Estos robots <a href="https://es.wikipedia.org/wiki/Antropomorfismo">antropom√≥rficos</a> se han popularizado en ficci√≥n como la pel√≠cula <a href="https://es.wikipedia.org/wiki/The_Terminator">The Terminator</a> y la caricatura <a href="https://es.wikipedia.org/wiki/Los_Supers%C3%B3nicos">Los Supers√≥nicos</a>. Pero los robots reales tienen muchas formas y tama√±os.</p> <p>Los <strong>manipuladores</strong> son s√≥lo brazos rob√≥ticos. No necesariamente tienen que estar unidos al cuerpo de un robot; podr√≠an simplemente atornillarse a una mesa o al suelo, como ocurre en las f√°bricas.</p> <p><img alt="File:Robotic ROV Manipulator Arm.png" src="assets/240px-Robotic_ROV_Manipulator_Arm.png"/></p> <p>Algunos tienen una gran carga √∫til, como los que ensamblan autom√≥viles, mientras que otros, como los brazos montables en sillas de ruedas que ayudan a las personas con discapacidades motoras, pueden transportar menos pero son m√°s seguros en entornos humanos.</p> <p>Los robots m√≥viles son aquellos que utilizan ruedas, patas o rotores para moverse por el entorno. Los drones cuadric√≥ptero son un tipo de veh√≠culo a√©reo no tripulado (<a href="https://es.wikipedia.org/wiki/Veh%C3%ADculo_a%C3%A9reo_no_tripulado">UAV</a>); Los veh√≠culos submarinos aut√≥nomos (<a href="https://es.wikipedia.org/wiki/Veh%C3%ADculo_submarino_aut%C3%B3nomo">AUV</a>) recorren los oc√©anos. Pero muchos robots m√≥viles permanecen en el interior y se mueven sobre ruedas, como una aspiradora o un robot repartidor de toallas en un hotel. Sus hom√≥logos al aire libre incluyen coches aut√≥nomos o rovers que exploran nuevos terrenos, incluso en la superficie de Marte.</p> <p><img alt="Nieuwe Marsrover van NASA stuurt adembenemende eerste foto's" src="assets/u=https%3A%2F%2Fcdn.businessinsider.nl%2Fwp-content%2Fuploads%2F2021%2F02%2F6035a5780270d.jpeg" style="zoom: 25%;"/></p> <p>Finalmente, los robots con patas est√°n destinados a atravesar terrenos accidentados a los que no se puede acceder con ruedas. La desventaja es que controlar las piernas para hacer lo correcto es m√°s desafiante que hacer girar las ruedas.</p> <p>Otros tipos de robots incluyen pr√≥tesis, exoesqueletos, robots con alas, enjambres y entornos inteligentes en los que el robot es toda la habitaci√≥n.</p> <h3 id="sintiendo-el-mundo">Sintiendo el mundo<a class="headerlink" href="#sintiendo-el-mundo" title="Permanent link">¬∂</a></h3> <p>Los sensores son la interfaz perceptiva entre el robot y el entorno. Los <strong>sensores pasivos</strong>, como las c√°maras, son verdaderos observadores del entorno: capturan se√±ales generadas por otras fuentes del entorno. Los <strong>sensores activos</strong>, como el sonar, env√≠an energ√≠a al medio ambiente. Se basan en el hecho de que esta energ√≠a se refleja de vuelta al sensor. Los sensores activos tienden a proporcionar m√°s informaci√≥n que los sensores pasivos, pero a costa de un mayor consumo de energ√≠a y con el peligro de interferencias cuando se utilizan varios sensores activos al mismo tiempo. Tambi√©n distinguimos si un sensor est√° dirigido a <strong>detectar el entorno</strong>, <strong>la ubicaci√≥n del robot</strong> o la <strong>configuraci√≥n interna del robot</strong>.</p> <p>Los tel√©metros son sensores que miden la distancia a objetos cercanos. Los sensores de sonar son tel√©metros activos que emiten ondas sonoras direccionales, que son reflejadas por los objetos y parte del sonido regresa al sensor. El tiempo y la intensidad de la se√±al de retorno indican la distancia a los objetos cercanos. El sonar es la tecnolog√≠a elegida para los veh√≠culos submarinos aut√≥nomos y fue popular en los primeros d√≠as de la rob√≥tica de interior. La <a href="https://es.wikipedia.org/wiki/Visi%C3%B3n_estereosc%C3%B3pica">visi√≥n estereosc√≥pica</a> se basa en m√∫ltiples c√°maras para obtener im√°genes del entorno desde puntos de vista ligeramente diferentes, analizando el paralaje resultante en estas im√°genes para calcular el rango de los objetos circundantes.</p> <p>Para los robots terrestres m√≥viles, el sonar y la visi√≥n est√©reo rara vez se utilizan ahora, porque no son confiablemente precisos. El Kinect es un sensor popular de bajo costo que combina una c√°mara y un proyector de luz estructurada, que proyecta un patr√≥n de l√≠neas de cuadr√≠cula en una escena. La c√°mara ve c√≥mo se doblan las l√≠neas de la cuadr√≠cula, d√°ndole al robot informaci√≥n sobre la forma de los objetos en la escena. Si se desea, la proyecci√≥n puede ser luz infrarroja, para no interferir con otros sensores (como los ojos humanos).</p> <p><img alt="Kinect for Windows SDK Programming Guide | Packt" src="assets/u=https%3A%2F%2Fstatic.packt-cdn.com%2Fproducts%2F9781849692380%2Fgraphics%2F2380_01_01.jpeg"/></p> <p>La mayor√≠a de los robots terrestres ahora est√°n equipados con tel√©metros √≥pticos activos. Al igual que los sensores de sonar, los sensores de alcance √≥ptico emiten se√±ales activas (luz) y miden el tiempo hasta que un reflejo de esta se√±al llega al sensor. La <a href="https://es.wikipedia.org/wiki/C%C3%A1mara_de_tiempo_de_vuelo">c√°mara de tiempo de vuelo</a> adquiere im√°genes de rango como la que se muestra m√°s abajo a hasta 60 fotogramas por segundo. Los autom√≥viles aut√≥nomos suelen utilizar <a href="https://es.wikipedia.org/wiki/LiDAR">lidares</a> de escaneo (abreviatura de detecci√≥n de luz y alcance): sensores activos que emiten rayos l√°ser y detectan el haz reflejado, brindando mediciones de alcance con una precisi√≥n de un cent√≠metro a una distancia de 100 metros. Utilizan complejas disposiciones de espejos o elementos giratorios para barrer el haz a trav√©s del entorno y construir un mapa. Los lidars de escaneo tienden a funcionar mejor que las c√°maras de tiempo de vuelo a distancias m√°s largas y tienden a funcionar mejor a plena luz del d√≠a.</p> <p><img alt="Why Choose time-of-flight for your Automotive 3D Sensing Applications" src="https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.melexis.com%2F-%2Fmedia%2Fimages%2Finsights%2Fwhy-choose-time-of-flight-for-your-automotive-3d-sensing-applications%2Ftime-of-flight---outputs.png&amp;f=1&amp;nofb=1&amp;ipt=0670338e71ace0224b5857aad8a6b0b9308762886403799428972b384b5d0055&amp;ipo=images"/></p> <p>El radar suele ser el sensor de determinaci√≥n de distancia elegido para los veh√≠culos a√©reos (aut√≥nomos o no). Los sensores de radar pueden medir distancias de hasta kil√≥metros y tienen la ventaja sobre los sensores √≥pticos de que pueden ver a trav√©s de la niebla. En el extremo cercano del rango de detecci√≥n se encuentran los sensores t√°ctiles, como bigotes, paneles de protuberancia y piel sensible al tacto. Estos sensores miden el alcance en funci√≥n del contacto f√≠sico y solo pueden implementarse para detectar objetos muy cerca del robot.</p> <p>Una segunda clase importante son los sensores de ubicaci√≥n. La mayor√≠a de los sensores de ubicaci√≥n utilizan la detecci√≥n de alcance como componente principal para determinar la ubicaci√≥n. En exteriores, el Sistema de Posicionamiento Global (GPS) es la soluci√≥n m√°s com√∫n al problema de localizaci√≥n. El GPS mide la distancia a los sat√©lites que emiten se√±ales pulsadas. Actualmente hay 31 sat√©lites <a href="https://es.wikipedia.org/wiki/GPS">GPS</a> operativos en √≥rbita y 24 sat√©lites <a href="https://es.wikipedia.org/wiki/GLONASS">GLONASS</a>, su hom√≥logo ruso. Los receptores GPS pueden recuperar la distancia a un sat√©lite analizando cambios de fase. Triangulando se√±ales de m√∫ltiples sat√©lites, los receptores GPS pueden determinar su ubicaci√≥n absoluta en la Tierra con una precisi√≥n de unos pocos metros. El GPS diferencial implica un segundo receptor terrestre con ubicaci√≥n conocida, que proporciona precisi√≥n milim√©trica en condiciones ideales.</p> <p>Desafortunadamente, el GPS no funciona en interiores ni bajo el agua. En interiores, la localizaci√≥n a menudo se logra colocando balizas en el entorno en lugares conocidos. Muchos entornos interiores est√°n llenos de estaciones base inal√°mbricas, que pueden ayudar a los robots a localizar mediante el an√°lisis de la se√±al inal√°mbrica. Las balizas de sonar activas bajo el agua pueden proporcionar una sensaci√≥n de ubicaci√≥n, utilizando el sonido para informar a los AUV de sus distancias relativas a esas balizas.</p> <p>La tercera clase importante son los sensores propioceptivos, que informan al robot de su propio movimiento. Para medir la configuraci√≥n exacta de una articulaci√≥n rob√≥tica, los motores suelen estar equipados con decodificadores de eje que miden con precisi√≥n el movimiento angular de un eje. En los brazos de los robots, los decodificadores de ejes ayudan a rastrear la posici√≥n de las articulaciones. En los robots m√≥viles, los decodificadores de eje informan las revoluciones de las ruedas para la odometr√≠a (la medici√≥n de la distancia recorrida). Desafortunadamente, las ruedas tienden a derrapar y patinar, por lo que la odometr√≠a s√≥lo es precisa en distancias cortas. Las fuerzas externas, como el viento y las corrientes oce√°nicas, aumentan la incertidumbre posicional. Los sensores inerciales, como los giroscopios, reducen la incertidumbre al confiar en la resistencia de la masa al cambio de velocidad.</p> <p>Otros aspectos importantes del estado del robot se miden mediante sensores de fuerza y ‚Äã‚Äãsensores de torsi√≥n. Son indispensables cuando los robots manipulan objetos fr√°giles o cuyo tama√±o y forma exactos se desconocen. Imagine un manipulador rob√≥tico de una tonelada enroscando una bombilla. Ser√≠a muy f√°cil aplicar demasiada fuerza y ‚Äã‚Äãromper la bombilla. Los sensores de fuerza le permiten al robot sentir con qu√© fuerza agarra la bombilla y los sensores de torsi√≥n le permiten sentir con qu√© fuerza gira. Los sensores de alta calidad pueden medir fuerzas en las tres direcciones de traslaci√≥n y tres direcciones de rotaci√≥n. Lo hacen a una frecuencia de varios cientos de veces por segundo para que un robot pueda detectar r√°pidamente fuerzas inesperadas y corregir sus acciones antes de que se rompa una bombilla. Sin embargo, puede ser un desaf√≠o equipar un robot con sensores de alta gama y la potencia computacional para monitorearlos.</p> <h3 id="produciendo-movimiento">Produciendo movimiento<a class="headerlink" href="#produciendo-movimiento" title="Permanent link">¬∂</a></h3> <p>El mecanismo que inicia el movimiento de un efector se llama actuador; los ejemplos incluyen transmisiones, engranajes, cables y varillajes. El tipo m√°s com√∫n de actuador es el <strong>actuador el√©ctrico</strong>, que utiliza electricidad para hacer girar un motor. Se utilizan predominantemente en sistemas que necesitan movimiento de rotaci√≥n, como las articulaciones de un brazo rob√≥tico. Los <strong>actuadores hidr√°ulicos</strong> utilizan fluido hidr√°ulico presurizado (como aceite o agua) y los <strong>actuadores neum√°ticos</strong> utilizan aire comprimido para generar movimiento mec√°nico.</p> <p>Los actuadores se utilizan a menudo para mover juntas que conectan cuerpos r√≠gidos (enlaces). Los brazos y las piernas tienen este tipo de articulaciones. En las articulaciones de revoluci√≥n, un eslab√≥n gira con respecto al otro. En las uniones prism√°ticas, un eslab√≥n se desliza a lo largo del otro. Ambas son articulaciones de un solo eje (un eje de movimiento). Otros tipos de articulaciones incluyen articulaciones esf√©ricas, cil√≠ndricas y planas, que son articulaciones multieje.</p> <p>Para interactuar con los objetos del entorno, los robots utilizan pinzas. El tipo m√°s b√°sico de pinza es la pinza de mand√≠bula paralela, con dos dedos y un √∫nico actuador que junta los dedos para agarrar objetos. Este efector es amado y odiado por su simplicidad.</p> <p><img alt="Shadow Hand &amp; Glove for Dextrous Manipulation | Shadow Robot" src="assets/u=https%3A%2F%2Fwww.shadowrobot.com%2Fwp-content%2Fuploads%2F2022%2F07%2FShadow-Robot-Co4266-copy-2-2.png"/></p> <p>Las pinzas de tres dedos ofrecen un poco m√°s de flexibilidad manteniendo la simplicidad. En el otro extremo del espectro est√°n las manos humanoides (antropom√≥rficas). Por ejemplo, Shadow Dexterous Hand tiene un total de 20 actuadores. Esto ofrece mucha m√°s flexibilidad para manipulaciones complejas, incluidas maniobras con manipuladores en la mano (piense en levantar su tel√©fono celular y girarlo en la mano para orientarlo hacia arriba), pero esta flexibilidad tiene un precio: aprender a controlar Estas pinzas complejas son m√°s desafiantes.</p> <h2 id="que-tipo-de-problema-resuelve-la-robotica">¬øQu√© tipo de problema resuelve la rob√≥tica?<a class="headerlink" href="#que-tipo-de-problema-resuelve-la-robotica" title="Permanent link">¬∂</a></h2> <p>Ahora que sabemos cu√°l podr√≠a ser el hardware del robot, estamos listos para considerar el software agente que impulsa el hardware para lograr nuestros objetivos. Primero debemos decidir el marco computacional para este agente.</p> <p>Ya hemos se√±alado que los problemas de la rob√≥tica no son deterministas, parcialmente observables y multiagentes. Podemos ver que a veces los agentes cooperan y otras son competitivos. En un pasillo estrecho donde s√≥lo un agente puede pasar primero, un robot y una persona colaboran porque ambos quieren asegurarse de no chocar entre s√≠. Pero en algunos casos pueden competir un poco para llegar r√°pidamente a su destino. Si el robot es demasiado educado y siempre deja espacio, puede quedarse atrapado en situaciones abarrotadas y nunca alcanzar su objetivo.</p> <p>¬øCu√°l es la funci√≥n de recompensa del robot en esta formulaci√≥n? Por lo general, el robot act√∫a al servicio de un ser humano; por ejemplo, entregando una comida a un paciente del hospital para obtener una recompensa del paciente, no la suya propia. Para la mayor√≠a de entornos de rob√≥tica, aunque los dise√±adores de robots podr√≠an intentar especificar una funci√≥n de recompensa suficientemente buena, la verdadera funci√≥n de recompensa recae en el usuario a quien se supone que el robot debe ayudar. El robot necesitar√° descifrar los deseos del usuario o confiar en un ingeniero para especificar una aproximaci√≥n a los deseos del usuario.</p> <p>En cuanto a los espacios de acci√≥n, estado y observaci√≥n del robot, la forma m√°s general es que las observaciones son se√±ales sin procesar de los sensores (por ejemplo, las im√°genes provenientes de las c√°maras o los impactos del l√°ser provenientes del lidar); las acciones son corrientes el√©ctricas brutas que se env√≠an a los motores; y el estado es lo que el robot necesita saber para tomar decisiones. Esto significa que existe una enorme brecha entre las percepciones de bajo nivel y los controles motores, y los planes de alto nivel que el robot necesita hacer. Para cerrar la brecha, los rob√≥ticos desacoplan aspectos del problema para simplificarlo.</p> <p>En rob√≥tica solemos utilizar una jerarqu√≠a de tres niveles. El nivel de <strong>planificaci√≥n de tareas</strong> decide un plan o pol√≠tica para acciones de alto nivel, a veces llamadas primitivas de acci√≥n o submetas: moverse hacia la puerta, abrirla, ir al ascensor, presionar el bot√≥n, etc. Luego, la <strong>planificaci√≥n de movimientos</strong> se encarga de encontrar un camino que lleva al robot de un punto a otro, logrando cada subobjetivo. Finalmente, el <strong>control</strong> se utiliza para lograr el movimiento planificado utilizando los actuadores del robot. Dado que el nivel de planificaci√≥n de tareas normalmente se define sobre estados y acciones discretos, en este cap√≠tulo nos centraremos principalmente en la planificaci√≥n y el control del movimiento.</p> <p>Por otra parte, el aprendizaje de preferencias se encarga de estimar el objetivo de un usuario final y la predicci√≥n de personas se utiliza para pronosticar las acciones de otras personas en el entorno del robot. Todos estos se combinan para determinar el comportamiento del robot.</p> <p>Siempre que dividimos un problema en partes separadas, reducimos la complejidad, pero renunciamos a oportunidades para que las partes se ayuden entre s√≠. La acci√≥n puede ayudar a mejorar la percepci√≥n y tambi√©n a determinar qu√© tipo de percepci√≥n es √∫til. De manera similar, las decisiones a nivel de movimiento podr√≠an no ser las mejores a la hora de tener en cuenta c√≥mo se dar√° seguimiento a ese movimiento; o las decisiones a nivel de tarea podr√≠an hacer que el plan de tarea no sea instanciable a nivel de movimiento. Entonces, con el progreso en estas √°reas separadas viene el impulso para reintegrarlas: planificar y controlar el movimiento juntos, planificar tareas y movimientos juntos, y reintegrar la percepci√≥n, la predicci√≥n y la acci√≥n, cerrando el ciclo de retroalimentaci√≥n. </p> <blockquote> <p>Hoy en d√≠a, la rob√≥tica consiste en seguir progresando en cada √°rea y, al mismo tiempo, aprovechar ese progreso para lograr una mejor integraci√≥n.</p> </blockquote> <h2 id="percepcion-robotica">Percepci√≥n rob√≥tica<a class="headerlink" href="#percepcion-robotica" title="Permanent link">¬∂</a></h2> <p>La percepci√≥n es el proceso mediante el cual los robots transforman las mediciones de los sensores en representaciones internas del entorno. Pero la percepci√≥n de la rob√≥tica debe lidiar con sensores adicionales como lidar y sensores t√°ctiles.</p> <p>La percepci√≥n es dif√≠cil porque los sensores son ruidosos y el entorno es parcialmente observable, impredecible y a menudo din√°mico. En otras palabras, los robots tienen todos los problemas de estimaci√≥n (o filtrado) de estado. Como regla general, una buena representaci√≥n interna de un robot tiene tres propiedades:</p> <ol> <li>Contienen suficiente informaci√≥n para que el robot tome buenas decisiones.</li> <li>Est√°n estructurados para que puedan actualizarse de manera eficiente.</li> <li>Son naturales en el sentido de que las variables internas corresponden a variables de estado natural en el mundo f√≠sico.</li> </ol> <p>Los <a href="https://es.wikipedia.org/wiki/Filtro_de_Kalman">filtros de Kalman</a>, los <a href="https://es.wikipedia.org/wiki/Modelo_oculto_de_M%C3%A1rkov">HMM</a> y las <a href="https://es.wikipedia.org/wiki/Red_bayesiana">redes de Bayes</a> pueden representar los modelos de transici√≥n y de sensor de un entorno parcialmente observable, y describimos algoritmos exactos y aproximados para actualizar el estado de creencia: la distribuci√≥n de probabilidad posterior sobre el entorno. Variables de estado. Para problemas de rob√≥tica, incluimos las acciones pasadas del propio robot como variables observadas en el modelo.</p> <h3 id="localizacion-y-mapeo">Localizaci√≥n y mapeo<a class="headerlink" href="#localizacion-y-mapeo" title="Permanent link">¬∂</a></h3> <p>La localizaci√≥n es el problema de descubrir d√≥nde est√°n las cosas, incluido el propio robot. Para simplificar las cosas, consideremos un robot m√≥vil que se mueve lentamente en un mundo plano bidimensional. Supongamos tambi√©n que al robot se le proporciona un mapa exacto del entorno. La pose de dicho robot m√≥vil se define por sus dos coordenadas cartesianas con valores <strong>x</strong> e <strong>y</strong> y su <strong>rumbo</strong> con valor <strong>Œ∏</strong>.</p> <p>En la aproximaci√≥n cinem√°tica, cada acci√≥n consiste en la especificaci√≥n ‚Äúinstant√°nea‚Äù de dos velocidades: una velocidad de traslaci√≥n <strong>vt</strong> y una velocidad de rotaci√≥n <strong>œât</strong>. Para intervalos de tiempo peque√±os <strong>‚àÜt</strong>, un modelo determinista crudo del movimiento de dichos robots viene dado por la notaci√≥n <strong>XÃÇ</strong> se refiere a una predicci√≥n de estado determinista. Por supuesto, los robots f√≠sicos son algo impredecibles. Esta distribuci√≥n de probabilidad es el modelo de movimiento del robot. Modela los efectos del movimiento en la ubicaci√≥n del robot.</p> <p>A continuaci√≥n, necesitamos un modelo de sensor. Consideraremos dos tipos de modelos de sensores. El primero supone que los sensores detectan caracter√≠sticas estables y reconocibles del entorno llamadas puntos de referencia. Para cada punto de referencia, se informa el alcance y el rumbo. Sin ruido, se puede calcular una predicci√≥n del alcance y el rumbo mediante geometr√≠a simple. Una vez m√°s, el ruido distorsiona nuestras mediciones. Para simplificar las cosas, supongamos ruido gaussiano con covarianza, lo que nos da el modelo del sensor.</p> <p>Se utiliza un modelo de sensor algo diferente para una matriz de sensores de alcance, cada uno de los cuales tiene un rumbo fijo con respecto al robot. Estos sensores producen un vector de valores. Dada una pose xt, el rango calculado a lo largo de la direcci√≥n del haz desde xt hasta el obst√°culo m√°s cercano. Como antes, esto se ver√° corrompido por el ruido gaussiano. Normalmente, asumimos que los errores para las diferentes direcciones del haz son independientes y est√°n distribuidos de manera id√©ntica.</p> <p>El filtro de Kalman, que representa el estado de creencia como un gaussiano multivariado √∫nico, y el filtro de part√≠culas, que representa el estado de creencia mediante una colecci√≥n de part√≠culas que corresponden a estados. La mayor√≠a de los algoritmos de localizaci√≥n modernos utilizan una de estas dos representaciones de la creencia del robot.</p> <p>La localizaci√≥n mediante filtrado de part√≠culas se denomina <strong><a href="https://en.wikipedia.org/wiki/Monte_Carlo_localization">localizaci√≥n de Monte Carlo</a></strong> o MCL. Todo lo que tenemos que hacer es proporcionar el modelo de movimiento y el modelo de sensor adecuados. El funcionamiento del algoritmo se se ve m√°s abajo. Cuando el robot descubre d√≥nde se encuentra dentro de un edificio de oficinas. En la primera imagen, las part√≠culas est√°n distribuidas uniformemente seg√∫n la anterior, lo que indica incertidumbre global sobre la posici√≥n del robot. En la segunda imagen, llega el primer conjunto de mediciones y las part√≠culas forman c√∫mulos en las zonas de alta creencia posterior. En el tercero, se dispone de suficientes mediciones para empujar todas las part√≠culas a un solo lugar.</p> <p><img alt="Adaptive Monte Carlo Localization - Robotics Knowledgebase" src="assets/u=https%3A%2F%2Froboticsknowledgebase.com%2Fwiki%2Fstate-estimation%2Fassets%2FAdaptiveMonteCarloLocalization-0d322.png"/></p> <p>El filtro de Kalman es la otra forma importante de localizaci√≥n. A medida que el robot se mueve, la incertidumbre en la estimaci√≥n de su ubicaci√≥n aumenta. Su error disminuye a medida que detecta el alcance y el rumbo hacia un punto de referencia con una ubicaci√≥n conocida y aumenta nuevamente cuando el robot pierde de vista el punto de referencia. Los algoritmos EKF funcionan bien si los puntos de referencia se identifican f√°cilmente.</p> <p>En algunas situaciones, no hay ning√∫n mapa del entorno disponible. Entonces el robot deber√° adquirir un mapa. Este es un problema del huevo y la gallina: el robot de navegaci√≥n tendr√° que determinar su ubicaci√≥n en relaci√≥n con un mapa que no conoce del todo y, al mismo tiempo, construir este mapa aunque no conozca su ubicaci√≥n real. Este problema es importante para muchas aplicaciones de robots y se ha estudiado ampliamente bajo el nombre de localizaci√≥n y mapeo simult√°neos, abreviado como <strong>SLAM</strong>.</p> <p>Los problemas de <strong>SLAM</strong> se resuelven utilizando muchas t√©cnicas probabil√≠sticas diferentes, incluido el filtro de Kalman extendido discutido anteriormente. Usar el EKF es sencillo: simplemente aumente el vector de estado para incluir las ubicaciones de los puntos de referencia en el entorno. Afortunadamente, la actualizaci√≥n de EKF escala cuadr√°ticamente, por lo que para mapas peque√±os (por ejemplo, unos pocos cientos de puntos de referencia) el c√°lculo es bastante factible. A menudo se obtienen mapas m√°s ricos utilizando m√©todos de relajaci√≥n de gr√°ficos, similares a las t√©cnicas de inferencia de redes bayesianas.</p> <h3 id="otros-tipos-de-percepcion">Otros tipos de percepci√≥n<a class="headerlink" href="#otros-tipos-de-percepcion" title="Permanent link">¬∂</a></h3> <p>No toda la percepci√≥n de los robots tiene que ver con la localizaci√≥n o el mapeo. Los robots tambi√©n perciben temperatura, olores, sonidos, etc. Muchas de estas cantidades pueden estimarse utilizando variantes de redes din√°micas de Bayes. Todo lo que se requiere para tales estimadores son distribuciones de probabilidad condicional que caractericen la evoluci√≥n de las variables de estado a lo largo del tiempo y modelos de sensores que describan la relaci√≥n de las mediciones con las variables de estado.</p> <p>La tendencia en rob√≥tica es claramente hacia representaciones con una sem√°ntica bien definida. Las t√©cnicas probabil√≠sticas superan a otros enfoques en muchos problemas de percepci√≥n dif√≠ciles, como la localizaci√≥n y el mapeo. Sin embargo, las t√©cnicas estad√≠sticas son a veces demasiado engorrosas y soluciones m√°s simples pueden ser igual de efectivas en la pr√°ctica.</p> <h3 id="aprendizaje-supervisado-y-no-supervisado-en-percepcion-de-robots">Aprendizaje supervisado y no supervisado en percepci√≥n de robots<a class="headerlink" href="#aprendizaje-supervisado-y-no-supervisado-en-percepcion-de-robots" title="Permanent link">¬∂</a></h3> <p>El aprendizaje autom√°tico juega un papel importante en la percepci√≥n de los robots. Este es particularmente el caso cuando no se conoce cu√°l es la mejor representaci√≥n interna. Un enfoque com√∫n es mapear flujos de sensores de alta dimensi√≥n en espacios de menor dimensi√≥n utilizando m√©todos de aprendizaje autom√°tico no supervisados. Este enfoque se denomina incrustaci√≥n de baja dimensi√≥n. El aprendizaje autom√°tico permite aprender modelos de sensores y de movimiento a partir de datos y, al mismo tiempo, descubrir una representaci√≥n interna adecuada.</p> <p>Otra t√©cnica de aprendizaje autom√°tico permite a los robots adaptarse continuamente a grandes cambios en las mediciones de los sensores. Imag√≠nese caminando desde un espacio iluminado por el sol hacia una habitaci√≥n oscura con luces de ne√≥n. Claramente, las cosas son m√°s oscuras por dentro. Pero el cambio de fuente de luz tambi√©n afecta a todos los colores: la luz de ne√≥n tiene un componente de luz verde m√°s fuerte que la luz solar. Sin embargo, de alguna manera parece que no notamos el cambio. Si entramos con personas en una habitaci√≥n iluminada con luces de ne√≥n, no creemos que sus caras de repente se pongan verdes. Nuestra percepci√≥n se adapta r√°pidamente a las nuevas condiciones de iluminaci√≥n y nuestro cerebro ignora las diferencias.</p> <p><img alt="Percepci√≥n adaptativa" src="assets/Screenshot_20240318_172942.png"/></p> <p>Las t√©cnicas de percepci√≥n adaptativa permiten a los robots adaptarse a tales cambios. En la Figura superior se muestra un ejemplo, tomado del √°mbito de la conducci√≥n aut√≥noma. Aqu√≠ un veh√≠culo terrestre no tripulado adapta su clasificador del concepto ‚Äúsuperficie transitable‚Äù. ¬øC√≥mo funciona esto? El robot utiliza un l√°ser para clasificar un √°rea peque√±a inmediatamente frente al robot. Cuando en el escaneo del alcance del l√°ser se encuentra que esta zona es plana, se utiliza como ejemplo de entrenamiento positivo para el concepto "superficie transitable". Luego se entrena una t√©cnica de mezcla de gaussianos, las im√°genes anteriores son el resultado de aplicar este clasificador a la imagen completa.</p> <p>Los m√©todos que hacen que los robots recopilen sus propios datos de entrenamiento (¬°con etiquetas!) se denominan autosupervisados. En este caso, el robot utiliza el aprendizaje autom√°tico para aprovechar un sensor de corto alcance que funciona bien para la clasificaci√≥n del terreno y convertirlo en un sensor que puede ver mucho m√°s lejos. Eso permite que el robot conduzca m√°s r√°pido y desacelere solo cuando el modelo del sensor indica que hay un cambio en el terreno que debe ser examinado m√°s cuidadosamente por los sensores de corto alcance.</p> <h2 id="planificacion-y-control">Planificaci√≥n y Control<a class="headerlink" href="#planificacion-y-control" title="Permanent link">¬∂</a></h2> <p>En √∫ltima instancia, las deliberaciones del robot se reducen a decidir c√≥mo moverse, desde el nivel de la tarea abstracta hasta las corrientes que se env√≠an a sus motores.</p> <p>Empezamos separando el movimiento del control. Definimos una trayectoria como una secuencia de puntos en el espacio geom√©trico que seguir√° un robot (o una parte de un robot, como un brazo). Esto est√° relacionado con la noci√≥n de camino, pero aqu√≠ nos referimos a una secuencia de puntos en el espacio m√°s que a una secuencia de acciones discretas. La tarea de encontrar un buen camino se llama planificaci√≥n del movimiento.</p> <p>Una vez que tenemos un camino, la tarea de ejecutar una secuencia de acciones para seguir el camino se llama control de seguimiento de trayectoria. Una trayectoria es un camino que tiene un tiempo asociado con cada punto del camino. Un camino simplemente dice "ir de A a B, a C, etc." y una trayectoria dice "empieza en A, toma 1 segundo para llegar a B y otros 1,5 segundos para llegar a C, etc."</p> <h3 id="espacio-de-configuracion">Espacio de configuraci√≥n<a class="headerlink" href="#espacio-de-configuracion" title="Permanent link">¬∂</a></h3> <p>Un espacio de configuraci√≥n, se refiere a un espacio abstracto que representa todas las posibles configuraciones o estados que un sistema puede adoptar. Esto es especialmente relevante en el contexto de sistemas aut√≥nomos como robots o agentes inteligentes, donde el sistema necesita tomar decisiones sobre qu√© acci√≥n tomar en funci√≥n de su entorno y su estado interno.</p> <p>En el caso de los robots, el espacio de configuraci√≥n podr√≠a incluir variables como la posici√≥n y orientaci√≥n del robot, las posibles velocidades y aceleraciones que puede alcanzar, las posiciones de los objetos en su entorno, entre otros factores relevantes para la tarea que debe realizar el robot.</p> <p>Entender y explorar el espacio de configuraci√≥n es fundamental para dise√±ar algoritmos y sistemas que puedan tomar decisiones efectivas y eficientes en diferentes situaciones.</p> <h3 id="planificacion-de-movimiento">Planificaci√≥n de movimiento<a class="headerlink" href="#planificacion-de-movimiento" title="Permanent link">¬∂</a></h3> <p>El problema de la planificaci√≥n del movimiento consiste en encontrar un plan que lleve a un robot de una configuraci√≥n a otra sin chocar con un obst√°culo. Es un componente b√°sico para el movimiento y la manipulaci√≥n. </p> <p>El problema de planificaci√≥n del movimiento a veces se denomina problema de la mudanza del piano. Debe su nombre a los esfuerzos de una empresa de mudanzas por llevar un piano grande y de forma irregular de una habitaci√≥n a otra sin golpear nada. Se nos da:</p> <ul> <li>un mundo de espacio de trabajo W (World),</li> <li>una regi√≥n de obst√°culo O ‚äÇ W (Obstacle),</li> <li>un robot con un espacio de configuraci√≥n C (Configuration) y un conjunto de puntos A(q) para q ‚àà C,</li> <li>una configuraci√≥n inicial qs ‚àà C, y (q start)</li> <li>una configuraci√≥n objetivo qg ‚àà C. (q goal)</li> </ul> <p>La regi√≥n del obst√°culo induce un obst√°culo en el espacio y su correspondiente espacio libre definido como en la secci√≥n anterior. Necesitamos encontrar un camino continuo a trav√©s del espacio libre. Usaremos una curva parametrizada, para representar el camino. </p> <p>El problema de la planificaci√≥n del movimiento puede hacerse m√°s complejo de varias maneras: definiendo el objetivo como un conjunto de configuraciones posibles en lugar de una configuraci√≥n √∫nica; definir una funci√≥n de costo (por ejemplo, longitud del camino) a minimizar; satisfacer las limitaciones (por ejemplo, si el camino implica llevar una taza de caf√©, asegurarse de que la taza est√© siempre orientada en posici√≥n vertical para que el caf√© no se derrame).</p> <p>Ahora consideremos algunas formas de resolver el problema de planificaci√≥n del movimiento.</p> <h4 id="graficos-de-visibilidad">Gr√°ficos de visibilidad<a class="headerlink" href="#graficos-de-visibilidad" title="Permanent link">¬∂</a></h4> <p><img alt="Gr√°ficos de visibilidad" src="assets/u=https%3A%2F%2Ftse1.mm.bing.net%2Fth%3Fid%3DOIP.jpeg"/></p> <p>Los gr√°ficos de visibilidad son una t√©cnica com√∫nmente utilizada en la planificaci√≥n de movimiento para robots m√≥viles. Estos gr√°ficos representan una estructura de datos que modela la conectividad entre diferentes puntos en un entorno, teniendo en cuenta las l√≠neas de visibilidad entre estos puntos. Son particularmente √∫tiles en entornos con obst√°culos donde se desea encontrar un camino libre de colisiones para un robot.</p> <p>Se utilizan del siguiente modo:</p> <ol> <li><strong>Representaci√≥n del entorno</strong>: Para crear un gr√°fico de visibilidad, primero se necesita una representaci√≥n del entorno en el que se mover√° el robot. Esta representaci√≥n puede ser en forma de un mapa discretizado, donde los obst√°culos se representan como √°reas intransitables, o puede ser en forma de una nube de puntos que representa los l√≠mites de los obst√°culos.</li> <li><strong>Puntos de visibilidad</strong>: Se identifican los puntos en el espacio que tienen l√≠neas de visibilidad claras entre ellos. Estos puntos generalmente incluyen v√©rtices de los obst√°culos y puntos de intersecci√≥n de l√≠neas de visi√≥n claras en el espacio libre entre los obst√°culos.</li> <li><strong>Construcci√≥n del grafo</strong>: Se construye un grafo donde los nodos representan los puntos de visibilidad y las aristas representan las l√≠neas de visi√≥n claras entre ellos. Es importante tener en cuenta que este grafo puede ser dirigido o no dirigido, dependiendo de la aplicaci√≥n espec√≠fica.</li> <li><strong>Algoritmos de b√∫squeda</strong>: Una vez que se ha construido el grafo de visibilidad, se pueden utilizar algoritmos de b√∫squeda como A* o Dijkstra para encontrar el camino m√°s corto y seguro entre dos puntos en el entorno. Estos algoritmos operan en el grafo de visibilidad y tienen en cuenta las restricciones de movimiento del robot y la presencia de obst√°culos.</li> <li><strong>Optimizaci√≥n y refinamiento</strong>: A menudo, los caminos encontrados por los algoritmos de b√∫squeda pueden ser sub√≥ptimos o no factibles debido a limitaciones del entorno o del robot. En este caso, se pueden aplicar t√©cnicas de optimizaci√≥n y refinamiento para mejorar el camino encontrado, como suavizado de trayectorias o replanificaci√≥n din√°mica.</li> </ol> <h4 id="diagramas-de-voronoi">Diagramas de Voronoi<a class="headerlink" href="#diagramas-de-voronoi" title="Permanent link">¬∂</a></h4> <p><img alt="Diagrama de Voronoi" src="assets/l251-voronoi-diagram-01-v4-732926563.png" style="zoom:33%;"/></p> <p>Estos gr√°ficos dividen un espacio en regiones que representan el √°rea m√°s cercana a un conjunto dado de puntos de control o sitios. En el contexto de la planificaci√≥n de movimiento para robots, los puntos de control suelen ser los obst√°culos en el entorno.</p> <p>La forma de utilizarlos es:</p> <ol> <li><strong>Identificaci√≥n de puntos de control</strong>: los puntos de control suelen ser los l√≠mites de los obst√°culos en el entorno. Estos puntos pueden ser esquinas, v√©rtices o cualquier punto distintivo en los l√≠mites de los obst√°culos.</li> <li><strong>Diagrama de Voronoi</strong>: Una vez que se han identificado los puntos de control, se construye el diagrama de Voronoi. Este diagrama divide el espacio en regiones donde cada regi√≥n est√° asociada con uno de los puntos de control y contiene todos los puntos que est√°n m√°s cerca de ese punto de control que de cualquier otro. Estas regiones se llaman celdas de Voronoi.</li> <li><strong>Gr√°fico de Voronoi</strong>: Las celdas de Voronoi se representan como pol√≠gonos, y los l√≠mites entre las celdas se representan como l√≠neas. Este gr√°fico proporciona una visualizaci√≥n clara de c√≥mo se divide el espacio en funci√≥n de la proximidad a los puntos de control.</li> <li><strong>Planificaci√≥n de trayectorias</strong>: Una vez que se ha construido el gr√°fico de Voronoi, se puede utilizar para planificar trayectorias seguras para el robot m√≥vil. Esto se logra encontrando un camino que minimice la distancia al punto de control m√°s cercano en cada punto de la trayectoria, lo que garantiza que el robot se mantenga lo m√°s alejado posible de los obst√°culos.</li> <li><strong>Optimizaci√≥n y refinamiento</strong>: A menudo, los caminos encontrados utilizando el gr√°fico de Voronoi pueden ser sub√≥ptimos o no factibles debido a restricciones adicionales del entorno o del robot. En estos casos, se pueden aplicar t√©cnicas de optimizaci√≥n y refinamiento para mejorar el camino encontrado, como suavizado de trayectorias o replanificaci√≥n din√°mica.</li> </ol> <h4 id="descomposicion-celular">Descomposici√≥n celular<a class="headerlink" href="#descomposicion-celular" title="Permanent link">¬∂</a></h4> <p><img alt="Descomposici√≥n celular" src="assets/image-20240321175518881.png" style="zoom: 50%;"/></p> <p>La descomposici√≥n celular (o celullar decomposition, en ingl√©s) se basa en dividir el espacio del entorno en regiones m√°s simples y manejables, lo que facilita la planificaci√≥n de trayectorias para el robot.</p> <p>Procedemos de la siguiente manera:</p> <ol> <li><strong>Representaci√≥n del entorno</strong>: Para utilizar la descomposici√≥n celular, primero necesitas una representaci√≥n del entorno en el que el robot se mover√°. Esto podr√≠a ser un mapa discreto donde los obst√°culos se representan como √°reas intransitables, o podr√≠a ser una nube de puntos que represente los l√≠mites de los obst√°culos.</li> <li><strong>Divisi√≥n del espacio</strong>: La descomposici√≥n celular divide el espacio del entorno en celdas m√°s peque√±as y manejables. Estas celdas pueden tener diferentes formas y tama√±os dependiendo del enfoque espec√≠fico de la descomposici√≥n utilizada. Una opci√≥n com√∫n es dividir el espacio en celdas regulares (por ejemplo, cuadrados en un plano 2D o cubos en un espacio 3D).</li> <li><strong>Conexiones entre celdas</strong>: Una vez que se han definido las celdas, se determinan las conexiones entre ellas. Esto implica identificar qu√© celdas son adyacentes entre s√≠ y, por lo tanto, podr√≠an ser transitables para el robot. Las conexiones pueden basarse en la geometr√≠a del entorno o en otros criterios relevantes para la planificaci√≥n de movimiento, como la distancia o la visibilidad entre celdas.</li> <li><strong>Creaci√≥n de un grafo de movimiento</strong>: Con las celdas y las conexiones definidas, se crea un grafo donde los nodos representan las celdas y las aristas representan las conexiones entre ellas. Este grafo se utiliza entonces para encontrar una trayectoria libre de colisiones para el robot, utilizando algoritmos de b√∫squeda como A* o Dijkstra.</li> <li><strong>Optimizaci√≥n y refinamiento</strong>: Una vez que se ha encontrado una trayectoria utilizando el grafo de movimiento, es posible que se requiera optimizaci√≥n y refinamiento adicionales para mejorar la calidad de la trayectoria. Esto podr√≠a implicar t√©cnicas como suavizado de trayectorias, replanificaci√≥n din√°mica o tener en cuenta restricciones espec√≠ficas del robot o del entorno.</li> </ol> <h4 id="planificacion-de-movimiento-aleatoria-probabilistic-roadmap-prm">Planificaci√≥n de movimiento aleatoria (Probabilistic RoadMap PRM)<a class="headerlink" href="#planificacion-de-movimiento-aleatoria-probabilistic-roadmap-prm" title="Permanent link">¬∂</a></h4> <p>Se basa en la generaci√≥n y evaluaci√≥n de trayectorias de manera aleatoria en el espacio de configuraci√≥n del robot. Aunque puede sonar simple, esta t√©cnica puede ser sorprendentemente efectiva en entornos complejos o desconocidos donde no es posible utilizar m√©todos deterministas debido a la falta de informaci√≥n precisa sobre el entorno o la presencia de obst√°culos din√°micos.</p> <p>Aqu√≠ hay una explicaci√≥n m√°s detallada de c√≥mo funciona la planificaci√≥n de movimiento aleatoria:</p> <ol> <li><strong>Inicializaci√≥n</strong>: La planificaci√≥n de movimiento aleatoria comienza con la inicializaci√≥n de una trayectoria de movimiento para el robot. Esto generalmente implica definir un punto de partida y un objetivo dentro del espacio de trabajo del robot.</li> <li><strong>Generaci√≥n de trayectorias aleatorias</strong>: A partir del punto de partida, se generan aleatoriamente una serie de posibles trayectorias de movimiento para el robot. Estas trayectorias pueden ser generadas de varias formas, como seleccionar puntos aleatorios dentro del espacio de trabajo y trazar una trayectoria entre ellos, o generando secuencias aleatorias de movimientos elementales que el robot puede realizar.</li> <li><strong>Evaluaci√≥n de las trayectorias</strong>: Cada trayectoria generada aleatoriamente se eval√∫a para determinar su calidad en funci√≥n de ciertos criterios. Estos criterios pueden incluir la longitud de la trayectoria, la cantidad de colisiones con obst√°culos, la distancia al objetivo, entre otros. Se pueden utilizar heur√≠sticas o funciones de coste para asignar una puntuaci√≥n a cada trayectoria en funci√≥n de estos criterios.</li> <li><strong>Selecci√≥n de la mejor trayectoria</strong>: Despu√©s de evaluar todas las trayectorias generadas aleatoriamente, se selecciona la mejor trayectoria seg√∫n los criterios establecidos. Esta trayectoria puede no ser la √≥ptima, pero se considera satisfactoria dadas las limitaciones del enfoque aleatorio.</li> <li><strong>Refinamiento y repetici√≥n</strong>: Una vez que se ha seleccionado una trayectoria, se puede realizar un refinamiento adicional para mejorar su calidad. Esto puede implicar suavizar la trayectoria, resolver colisiones potenciales o realizar ajustes para adaptarse mejor al entorno. Adem√°s, si la trayectoria no es satisfactoria, el proceso puede repetirse generando y evaluando nuevas trayectorias aleatorias hasta que se encuentre una soluci√≥n aceptable.</li> </ol> <h4 id="arboles-aleatorios-que-se-exploran-rapidamente">√Årboles aleatorios que se exploran r√°pidamente<a class="headerlink" href="#arboles-aleatorios-que-se-exploran-rapidamente" title="Permanent link">¬∂</a></h4> <p>RRT (Rapidly-exploring Random Trees) es una variante del algoritmo busca generar un camino entre un punto inicial y un punto objetivo explorando desde ambos extremos simult√°neamente. Aqu√≠ te explico c√≥mo funciona:</p> <ol> <li><strong>Inicializaci√≥n</strong>: Se inicia con dos √°rboles, uno que comienza en el punto inicial y otro que comienza en el punto objetivo. Cada √°rbol consta de un solo nodo que representa su respectivo punto de partida.</li> <li><strong>Expansi√≥n aleatoria</strong>: En cada iteraci√≥n del algoritmo, se genera aleatoriamente un nuevo punto en el espacio de configuraci√≥n, uno para cada √°rbol. Estos puntos aleatorios son generados de manera similar a como se describe en el RRT est√°ndar, posiblemente influenciados por la posici√≥n del otro punto objetivo.</li> <li><strong>Extensi√≥n de √°rboles</strong>: Cada nuevo punto generado se conecta al nodo m√°s cercano en el √°rbol correspondiente. Esto implica que cada √°rbol se expande hacia el punto generado. Se verifica la validez de las conexiones y se a√±aden los nuevos nodos al √°rbol si la conexi√≥n es viable.</li> <li><strong>Colisi√≥n de √°rboles</strong>: En cada iteraci√≥n, se verifica si los √°rboles se han encontrado entre s√≠. Esto se puede hacer comprobando si un nuevo nodo generado en un √°rbol se encuentra dentro de un cierto radio de cercan√≠a de un nodo en el otro √°rbol. Si se encuentran, se ha encontrado una posible soluci√≥n.</li> <li><strong>Construcci√≥n del camino</strong>: Una vez que los dos √°rboles se han encontrado, se puede construir un camino entre el punto inicial y el punto objetivo. Esto se hace trazando una trayectoria desde el nodo del √°rbol inicial hasta el nodo del √°rbol objetivo, combinando las trayectorias de ambos √°rboles.</li> <li><strong>Optimizaci√≥n y refinamiento</strong>: Despu√©s de encontrar un camino inicial, se puede realizar una optimizaci√≥n adicional para mejorar su calidad. Esto puede implicar suavizar la trayectoria, resolver colisiones potenciales o eliminar trayectorias redundantes.</li> <li><strong>Finalizaci√≥n y selecci√≥n de la mejor trayectoria</strong>: Una vez que se ha encontrado una trayectoria satisfactoria, se selecciona como la soluci√≥n del problema de planificaci√≥n de movimiento.</li> </ol> <h4 id="optimizacion-de-trayectoria-para-planificacion-cinematica">Optimizaci√≥n de trayectoria para planificaci√≥n cinem√°tica<a class="headerlink" href="#optimizacion-de-trayectoria-para-planificacion-cinematica" title="Permanent link">¬∂</a></h4> <p>La optimizaci√≥n de trayectoria en la planificaci√≥n cinem√°tica para robots se refiere al proceso de mejorar una trayectoria generada inicialmente para que cumpla con ciertos criterios espec√≠ficos, como minimizar el tiempo de ejecuci√≥n, reducir la energ√≠a consumida, evitar colisiones o maximizar la suavidad de la trayectoria. Aqu√≠ est√°n los pasos principales involucrados en la optimizaci√≥n de trayectorias para la planificaci√≥n de movimiento de robots:</p> <ol> <li><strong>Definici√≥n de la funci√≥n objetivo</strong>: Antes de iniciar la optimizaci√≥n, es necesario definir una funci√≥n objetivo que capture los objetivos de optimizaci√≥n deseados. Esta funci√≥n puede tener varios componentes, como la distancia recorrida, el tiempo de ejecuci√≥n, la energ√≠a consumida, la suavidad de la trayectoria o la distancia m√≠nima a los obst√°culos.</li> <li><strong>Formulaci√≥n del problema de optimizaci√≥n</strong>: Una vez que se ha definido la funci√≥n objetivo, el problema de optimizaci√≥n se formula para encontrar la trayectoria que minimiza o maximiza esta funci√≥n objetivo, sujeto a ciertas restricciones. Estas restricciones pueden incluir limitaciones en la velocidad, aceleraci√≥n o jerarqu√≠as cinem√°ticas del robot, as√≠ como restricciones de colisi√≥n con el entorno.</li> <li><strong>Selecci√≥n de algoritmo de optimizaci√≥n</strong>: Existen diversos algoritmos de optimizaci√≥n que pueden utilizarse para resolver el problema formulado. Algunos de los algoritmos com√∫nmente utilizados incluyen el m√©todo del gradiente descendente, algoritmos de b√∫squeda heur√≠stica como algoritmos gen√©ticos, optimizaci√≥n basada en enjambres de part√≠culas (PSO), optimizaci√≥n por enjambre de hormigas (ACO), entre otros.</li> <li><strong>Aplicaci√≥n del algoritmo de optimizaci√≥n</strong>: Una vez seleccionado el algoritmo adecuado, se aplica para encontrar la trayectoria que optimiza la funci√≥n objetivo. Esto implica ejecutar el algoritmo en iteraciones sucesivas, donde en cada iteraci√≥n se ajusta la trayectoria actual en funci√≥n de la funci√≥n objetivo y las restricciones.</li> <li><strong>Evaluaci√≥n de la soluci√≥n obtenida</strong>: Despu√©s de que el algoritmo de optimizaci√≥n converge o alcanza un cierto criterio de finalizaci√≥n, se eval√∫a la soluci√≥n obtenida para asegurarse de que cumple con los requisitos deseados. Esto puede implicar la simulaci√≥n de la trayectoria planificada en un entorno virtual para verificar la ausencia de colisiones y la suavidad de la trayectoria.</li> <li><strong>Refinamiento y ajuste</strong>: Si es necesario, se pueden realizar ajustes adicionales en la trayectoria obtenida para mejorar su calidad. Esto puede incluir t√©cnicas de suavizado de trayectorias para reducir las discontinuidades, ajustes locales para evitar colisiones o cambios en la parametrizaci√≥n de la trayectoria para optimizarla a√∫n m√°s.</li> </ol> <h3 id="control-de-seguimiento-de-trayectoria">Control de seguimiento de trayectoria<a class="headerlink" href="#control-de-seguimiento-de-trayectoria" title="Permanent link">¬∂</a></h3> <p>El control de seguimiento de trayectoria en el movimiento de robots se encarga de guiar al robot para que siga una trayectoria planificada lo m√°s cercanamente posible, considerando limitaciones del sistema y perturbaciones externas. Este proceso implica varios pasos:</p> <ol> <li><strong>Obtenci√≥n de la trayectoria planificada</strong>: Se adquiere la trayectoria deseada que se espera que el robot siga.</li> <li><strong>Realimentaci√≥n de la informaci√≥n del sistema</strong>: Se recopila informaci√≥n en tiempo real sobre el estado del robot, como su posici√≥n, velocidad y orientaci√≥n.</li> <li><strong>Comparaci√≥n con la trayectoria planificada</strong>: Se compara continuamente el estado actual del robot con la trayectoria planificada para determinar el error de seguimiento.</li> <li><strong>Dise√±o del controlador</strong>: Se dise√±a un controlador que genere comandos de control para minimizar el error y guiar al robot hacia la trayectoria deseada.</li> <li><strong>Aplicaci√≥n de los comandos de control</strong>: Los comandos de control se aplican al sistema del robot para influir en su movimiento y lograr que siga la trayectoria planificada.</li> <li><strong>Realimentaci√≥n y ajuste continuo</strong>: Se ajustan los comandos de control en cada ciclo de control para mantener al robot en la trayectoria deseada, incluso en presencia de perturbaciones.</li> </ol> <p>Para lograr este seguimiento de trayectoria, se utilizan diferentes tipos de controladores, como:</p> <ul> <li><strong>Controlador Proporcional</strong> (P): Aplica fuerza en proporci√≥n negativa al error observado entre la posici√≥n real y deseada del robot.</li> <li><strong>Controlador Proporcional-Derivado</strong> (PD): Extiende el control proporcional al agregar un t√©rmino que es proporcional a la primera derivada del error a lo largo del tiempo, lo que amortigua el sistema controlado.</li> <li><strong>Controlador Proporcional-Integral-Derivado</strong> (PID): Incluye un t√©rmino adicional que integra el error en el tiempo, lo que ayuda a corregir errores sistem√°ticos prolongados.</li> <li><strong>Control de Par Calculado</strong>: Combina la din√°mica inversa con la realimentaci√≥n del error para calcular el par necesario que el modelo del robot cree que se requiere, compensando la inexactitud del modelo con t√©rminos de error proporcionales.</li> </ul> <h4 id="planes-versus-politicas">Planes versus pol√≠ticas<a class="headerlink" href="#planes-versus-politicas" title="Permanent link">¬∂</a></h4> <p>Con el movimiento en rob√≥tica, en realidad estamos considerando un MDP (Markov Decision Process) subyacente donde los estados son estados din√°micos (configuraci√≥n y velocidad) y las acciones son entradas de control, generalmente en forma de pares. Si echas otro vistazo a nuestras leyes de control anteriores, son pol√≠ticas, no planes: le dicen al robot qu√© acci√≥n tomar desde cualquier estado al que pueda llegar. Sin embargo, suelen estar lejos de ser pol√≠ticas √≥ptimas. Debido a que el estado din√°mico es continuo y de alta dimensi√≥n (al igual que el espacio de acci√≥n), las pol√≠ticas √≥ptimas son computacionalmente dif√≠ciles de extraer.</p> <p>En cambio, lo que hicimos aqu√≠ fue solucionar el problema. Primero elaboramos un plan, en un estado y un espacio de acci√≥n simplificados: usamos solo el estado cinem√°tico y asumimos que los estados son alcanzables entre s√≠ sin prestar atenci√≥n a la din√°mica subyacente. Esta es la planificaci√≥n del movimiento y nos da la ruta de referencia.</p> <p>Pero como nuestro modelo din√°mico suele ser err√≥neo, lo convertimos en una pol√≠tica que intenta seguir el plan, volviendo a √©l cuando se aleja. Al hacer esto, introducimos suboptimidad de dos maneras: primero, planificando sin considerar la din√°mica y, segundo, asumiendo que si nos desviamos del plan, lo √≥ptimo es volver al plan original. A continuaci√≥n, describimos t√©cnicas que calculan pol√≠ticas directamente sobre el estado din√°mico, evitando la separaci√≥n por completo.</p> <h3 id="control-optimo">Control √≥ptimo<a class="headerlink" href="#control-optimo" title="Permanent link">¬∂</a></h3> <p>En lugar de utilizar un planificador para crear una ruta cinem√°tica y preocuparse √∫nicamente por la din√°mica del sistema despu√©s del hecho, aqu√≠ analizamos c√≥mo podr√≠amos hacerlo todo a la vez. Tomaremos el problema de optimizaci√≥n de trayectorias para rutas cinem√°ticas y lo convertiremos en una verdadera optimizaci√≥n de trayectorias con din√°mica: optimizaremos directamente sobre las acciones, teniendo en cuenta la din√°mica (o transiciones).</p> <p>El enfoque propuesto combina la planificaci√≥n de trayectorias con la din√°mica del sistema en una sola optimizaci√≥n. Se busca una secuencia de acciones que minimice un costo acumulado, teniendo en cuenta la transici√≥n del estado del sistema y las restricciones. Esto se relaciona con la planificaci√≥n de movimiento y el control de seguimiento de trayectorias al considerar configuraciones y acciones simult√°neamente.</p> <p>Para resolver este problema, se pueden tomar gradientes del costo acumulado con respecto a las acciones. Se utilizan t√©cnicas de optimizaci√≥n de trayectorias como el tiroteo m√∫ltiple y la colocaci√≥n directa. Cuando el costo es cuadr√°tico y la din√°mica es lineal, se puede aplicar el regulador cuadr√°tico lineal (LQR), que encuentra una pol√≠tica √≥ptima de forma eficiente. Aunque los problemas reales rara vez cumplen estas condiciones, el LQR y su variante ILQR se utilizan ampliamente en la pr√°ctica.</p> <h2 id="planificacion-de-movimientos-inciertos">Planificaci√≥n de movimientos inciertos<a class="headerlink" href="#planificacion-de-movimientos-inciertos" title="Permanent link">¬∂</a></h2> <p>En rob√≥tica, la incertidumbre surge de la observabilidad parcial del entorno y de los efectos estoc√°sticos (o no modelados) de las acciones del robot. Tambi√©n pueden surgir errores por el uso de algoritmos de aproximaci√≥n, como el filtrado de part√≠culas, que no le dan al robot un estado de creencia exacto incluso si el entorno est√° modelado perfectamente.</p> <p>La mayor√≠a de los robots actuales utilizan algoritmos deterministas para la toma de decisiones, como los algoritmos de planificaci√≥n de ruta de la secci√≥n anterior o los algoritmos de b√∫squeda. Estos algoritmos deterministas se adaptan de dos maneras: en primer lugar, se ocupan de la espacio de estado continuo convirti√©ndolo en un espacio discreto (por ejemplo, con gr√°ficos de visibilidad o descomposici√≥n de celdas). En segundo lugar, abordan la incertidumbre en el estado actual eligiendo el estado m√°s probable a partir de la distribuci√≥n de probabilidad producida por el algoritmo de estimaci√≥n del estado. Ese enfoque hace que el c√°lculo sea m√°s r√°pido y se adapta mejor a los algoritmos de b√∫squeda deterministas. En esta secci√≥n analizamos m√©todos para abordar la incertidumbre.</p> <ol> <li><strong>Replanificaci√≥n en l√≠nea:</strong> En entornos inciertos, los planes deterministas pueden volverse sub√≥ptimos. La replanificaci√≥n en l√≠nea, como el control predictivo de modelos (MPC), permite recalcular continuamente planes basados en nueva informaci√≥n. Esto se logra planificando para un horizonte temporal corto y ajustando los planes en cada paso del tiempo.</li> <li><strong>Acciones de recopilaci√≥n de informaci√≥n:</strong> La incertidumbre tambi√©n requiere acciones espec√≠ficas para recopilar informaci√≥n relevante. En lugar de separar la estimaci√≥n del control, se puede resolver un Proceso de Decisi√≥n de Markov Parcialmente Observado (POMDP) para considerar la incertidumbre en la planificaci√≥n. Esto permite que el robot razone sobre la informaci√≥n futura que podr√≠a obtener y tome acciones √≥ptimas considerando tanto la informaci√≥n actual como la futura.</li> </ol> <h2 id="aprendizaje-por-refuerzo-en-robotica">Aprendizaje por refuerzo en rob√≥tica<a class="headerlink" href="#aprendizaje-por-refuerzo-en-robotica" title="Permanent link">¬∂</a></h2> <p>Hasta ahora hemos considerado tareas en las que el robot tiene acceso al modelo din√°mico del mundo. En muchas tareas, es muy dif√≠cil escribir un modelo de este tipo, lo que nos sit√∫a en el dominio del aprendizaje por refuerzo (RL).</p> <p>Un desaf√≠o de RL en rob√≥tica es la naturaleza continua de los espacios de estado y acci√≥n, que manejamos mediante discretizaci√≥n o, m√°s com√∫nmente, mediante aproximaci√≥n de funciones. Las pol√≠ticas o funciones de valor se representan como combinaciones de caracter√≠sticas √∫tiles conocidas o como redes neuronales profundas. Las redes neuronales pueden mapear desde entradas sin procesar directamente a salidas y, por lo tanto, evitan en gran medida la necesidad de ingenier√≠a de caracter√≠sticas, pero requieren m√°s datos.</p> <p>Un desaf√≠o mayor es que los robots operen en el mundo real. Hemos visto c√≥mo se puede utilizar el aprendizaje por refuerzo para aprender a jugar al ajedrez o al Go jugando juegos simulados. Pero cuando un robot real se mueve en el mundo real, tenemos que asegurarnos de que sus acciones sean seguras (¬°las cosas se rompen!), y tenemos que aceptar que el progreso ser√° m√°s lento que en una simulaci√≥n porque el mundo se niega a moverse m√°s r√°pido que un segundo por segundo. Gran parte de lo interesante del uso del aprendizaje por refuerzo en rob√≥tica se reduce a c√≥mo podemos reducir la complejidad de las muestras del mundo real: el n√∫mero de interacciones con el mundo f√≠sico que el robot necesita antes de aprender a realizar la tarea.</p> <h2 id="humanos-y-robots">Humanos y robots<a class="headerlink" href="#humanos-y-robots" title="Permanent link">¬∂</a></h2> <p>Hasta ahora, nos hemos centrado en la planificaci√≥n de un robot y en aprender a actuar de forma aislada. Esto es √∫til para algunos robots, como los rovers que enviamos a explorar planetas distantes en nuestro nombre. Pero, en general, no construimos robots para que funcionen de forma aislada. Los construimos para ayudarnos y para trabajar en entornos humanos, a nuestro alrededor y con nosotros.</p> <p>Esto plantea dos desaf√≠os complementarios. El primero es optimizar la recompensa cuando hay personas actuando en el mismo entorno que el robot. A esto lo llamamos problema de coordinaci√≥n. Cuando la recompensa del robot depende no s√≥lo de sus propias acciones, sino tambi√©n de las acciones que realizan las personas, el robot tiene que elegir sus acciones de una manera que combine bien con las de ellos. Cuando el humano y el robot est√°n en el mismo equipo, esto se convierte en colaboraci√≥n.</p> <p>En segundo lugar est√° el desaf√≠o de optimizar lo que la gente realmente quiere. Si un robot va a ayudar a las personas, su funci√≥n de recompensa debe incentivar las acciones que las personas quieren que ejecute el robot. Determinar la funci√≥n (o pol√≠tica) de recompensa adecuada para el robot es en s√≠ mismo un problema de interacci√≥n. Exploraremos estos dos desaf√≠os uno por uno.</p> <h3 id="coordinacion">Coordinaci√≥n<a class="headerlink" href="#coordinacion" title="Permanent link">¬∂</a></h3> <p>Supongamos por ahora, como hasta ahora, que el robot tiene acceso a una funci√≥n de recompensa claramente definida. Pero, en lugar de necesitar optimizarlo de forma aislada, ahora el robot necesita optimizarlo en torno a un humano que tambi√©n act√∫a. Por ejemplo, cuando un autom√≥vil aut√≥nomo se incorpora a la autopista, necesita negociar la maniobra con el conductor humano que viene al carril objetivo: ¬ødeber√≠a acelerar y incorporarse al frente, o reducir la velocidad y incorporarse a la parte trasera? M√°s tarde, cuando se detiene ante una se√±al de stop, prepar√°ndose para girar a la derecha, tiene que tener cuidado con el ciclista en el carril bici y con el peat√≥n que est√° a punto de pisar el paso de peatones.</p> <p>O considere un robot m√≥vil en un pasillo. Alguien que se dirige directamente hacia el robot da un paso ligeramente hacia la derecha, indicando por qu√© lado del robot quiere pasar. El robot tiene que responder, aclarando sus intenciones.</p> <h4 id="los-humanos-como-agentes-aproximadamente-racionales">Los humanos como agentes aproximadamente racionales<a class="headerlink" href="#los-humanos-como-agentes-aproximadamente-racionales" title="Permanent link">¬∂</a></h4> <p>Una forma de formular la coordinaci√≥n con un humano es modelarla como un juego entre el robot y el humano. Con este enfoque, asumimos expl√≠citamente que las personas son agentes incentivados por objetivos. Esto no significa autom√°ticamente que sean agentes perfectamente racionales (es decir, que encuentren soluciones √≥ptimas en el juego), pero s√≠ significa que el robot puede estructurar la forma en que razona sobre el humano a trav√©s de la noci√≥n de posibles objetivos que el humano podr√≠a tener.</p> <p>Tres aspectos importantes complican este juego. La <strong>primera</strong> es que el humano y el robot no necesariamente conocen los objetivos del otro. Esto lo convierte en un juego de informaci√≥n incompleta.</p> <p>En <strong>segundo</strong> lugar, los espacios de estado y acci√≥n son continuos. Podemos hacer una b√∫squeda en √°rbol para abordar juegos discretos, pero ¬øc√≥mo abordamos espacios continuos?</p> <p>En <strong>tercer</strong> lugar, aunque en el nivel alto el modelo de juego tiene sentido (los humanos se mueven y tienen objetivos), es posible que el comportamiento de un humano no siempre est√© bien caracterizado como una soluci√≥n al juego. El juego supone un desaf√≠o computacional no s√≥lo para el robot, sino tambi√©n para nosotros, los humanos. Requiere pensar en lo que har√° el robot en respuesta a lo que hace la persona, lo cual depende de lo que el robot cree que har√° la persona, y muy pronto llegamos a "¬øqu√© crees que creo que creo que pienso?": son las tortugas. ¬°toda la calle abajo! Los humanos no pueden lidiar con todo eso y exhiben ciertas sub√≥ptimos. Esto significa que el robot debe tener en cuenta estas sub√≥ptimas.</p> <p>Entonces, ¬øqu√© debe hacer un coche aut√≥nomo cuando el problema de coordinaci√≥n es tan dif√≠cil? Tomaremos el juego y lo dividiremos en hacer predicciones sobre las acciones humanas y decidir qu√© deber√≠a hacer el robot dadas estas predicciones.</p> <h4 id="predecir-la-accion-humana">Predecir la acci√≥n humana<a class="headerlink" href="#predecir-la-accion-humana" title="Permanent link">¬∂</a></h4> <p>Predecir las acciones humanas es dif√≠cil porque dependen de las acciones del robot y viceversa. Un truco que utilizan los robots es fingir que la persona est√° ignorando al robot. El robot supone que las personas son √≥ptimas con respecto a su objetivo, que el robot desconoce y que se modela como si ya no dependiera de las acciones del robot. </p> <p>As√≠ es como las acciones pasadas del humano acaban informando al robot sobre lo que el humano har√° en el futuro. Tener una creencia sobre el objetivo del ser humano ayuda al robot a anticipar las pr√≥ximas acciones que realizar√° el ser humano.</p> <p>Lo mismo puede suceder al conducir. Puede que no sepamos cu√°nto valora otro conductor la eficiencia, pero si lo vemos acelerar cuando alguien intenta incorporarse delante de √©l, ahora sabemos un poco m√°s sobre √©l. Y una vez que sepamos eso, podremos anticipar mejor lo que har√°n en el futuro: es probable que el mismo conductor se acerque m√°s a nosotros o se abra paso entre el tr√°fico para adelantarnos.</p> <h4 id="predicciones-humanas-sobre-el-robot">Predicciones humanas sobre el robot.<a class="headerlink" href="#predicciones-humanas-sobre-el-robot" title="Permanent link">¬∂</a></h4> <p>La informaci√≥n incompleta suele tener dos caras: el robot no conoce el objetivo del ser humano y el ser humano, a su vez, no conoce el objetivo del robot; es necesario que la gente haga predicciones sobre los robots. Como dise√±adores de robots, no estamos a cargo de c√≥mo el ser humano hace predicciones; s√≥lo podemos controlar lo que hace el robot. Sin embargo, el robot puede actuar de manera que al humano le resulte m√°s f√°cil hacer predicciones correctas. El robot puede suponer que el humano est√° usando algo m√°s o menos an√°logo a la ecuaci√≥n para estimar el objetivo del robot y, por lo tanto, el robot actuar√° de manera que su verdadero objetivo pueda inferirse f√°cilmente.</p> <p>Un caso especial del juego es cuando el humano y el robot est√°n en el mismo equipo, trabajando hacia la misma meta u objetivo. Imag√≠nese tener un robot dom√©stico personal que lo ayude a preparar la cena o limpiar; estos son ejemplos de colaboraci√≥n.</p> <p>Ahora podemos definir un agente conjunto cuyas acciones son tuplas de acciones humano-robot y que optimiza para, y estamos resolviendo un problema de planificaci√≥n habitual. Calculamos el plan o pol√≠tica √≥ptimo para el agente conjunto y listo, ahora sabemos qu√© deben hacer el robot y el humano.</p> <p>Esto funcionar√≠a muy bien si las personas fueran perfectamente √≥ptimas. El robot har√≠a su parte del plan conjunto, el humano la suya. Desafortunadamente, en la pr√°ctica, la gente no parece seguir el plan de agente conjunto perfectamente dise√±ado; ¬°Tienen opini√≥n propia! Sin embargo, ya hemos aprendido una forma de manejar esto con el control predictivo de modelo (MPC): la idea es idear un plan, ejecutar la primera acci√≥n y luego volver a planificar. De esta manera, el robot siempre adapta su plan a lo que realmente est√° haciendo el humano.</p> <blockquote> <p>Supongamos que usted y el robot est√°n en su cocina y han decidido hacer gofres. Est√°s un poco m√°s cerca del frigor√≠fico, por lo que el plan conjunto √≥ptimo ser√≠a coger los huevos y la leche del frigor√≠fico, mientras el robot recoge la harina del armario. El robot lo sabe porque puede medir con bastante precisi√≥n d√≥nde est√° cada uno. Pero supongamos que empiezas a dirigirte al gabinete de harina. Est√°s yendo en contra del plan conjunto √≥ptimo. En lugar de ce√±irse a ello y obstinadamente ir tambi√©n a por la harina, el robot recalcula el plan √≥ptimo, y ahora que est√°s lo suficientemente cerca de la harina, lo mejor es que el robot agarre la plancha para gofres.</p> <p>Si sabemos que las personas podr√≠an desviarse del √≥ptimo, podemos dar cuenta de ello con anticipaci√≥n. El robot puede intentar anticipar que vas a por la harina en el momento en que das el primer paso (digamos, usando la t√©cnica de predicci√≥n anterior). Aunque t√©cnicamente sigue siendo √≥ptimo que te des la vuelta y te dirijas al frigor√≠fico, el robot no deber√≠a asumir que eso es lo que va a pasar. En cambio, el robot puede calcular un plan en el que usted sigue haciendo lo que parece querer.</p> </blockquote> <h3 id="aprender-a-hacer-lo-que-los-humanos-quieren">Aprender a hacer lo que los humanos quieren<a class="headerlink" href="#aprender-a-hacer-lo-que-los-humanos-quieren" title="Permanent link">¬∂</a></h3> <p>Otra forma en que la interacci√≥n con los humanos entra en la rob√≥tica es en la propia JR: la funci√≥n de costo o recompensa del robot. El marco de agentes racionales y los algoritmos asociados reducen el problema de generar un buen comportamiento a especificar una buena funci√≥n de recompensa. Pero para los robots, como para muchos otros agentes de IA, todav√≠a es dif√≠cil calcular correctamente el costo.</p> <p>Tomemos como ejemplo los autom√≥viles aut√≥nomos: queremos que lleguen al destino, que sean seguros, que conduzcan c√≥modamente para sus pasajeros, que obedezcan las leyes de tr√°nsito, etc. Un dise√±ador de un sistema de este tipo necesita equilibrar estos diferentes componentes de la funci√≥n de costos. La tarea del dise√±ador es dif√≠cil porque los robots est√°n dise√±ados para ayudar a los usuarios finales y no todos los usuarios finales son iguales. Todos tenemos diferentes preferencias sobre la agresividad con la que queremos que conduzca nuestro coche, etc.</p> <p>A continuaci√≥n, exploramos dos alternativas para intentar que el comportamiento del robot coincida con lo que realmente queremos que haga. La primera es aprender una funci√≥n de costos a partir del aporte humano. La segunda es evitar la funci√≥n de costos e imitar las demostraciones humanas de la tarea.</p> <h4 id="aprendizaje-preferencial-funciones-de-costo-de-aprendizaje">Aprendizaje preferencial: funciones de costo de aprendizaje<a class="headerlink" href="#aprendizaje-preferencial-funciones-de-costo-de-aprendizaje" title="Permanent link">¬∂</a></h4> <p>Imagine que un usuario final le muestra a un robot c√≥mo realizar una tarea. Por ejemplo, conducen el coche de la forma que les gustar√≠a que lo condujera el robot. ¬øSe te ocurre alguna manera de que el robot utilice estas acciones (las llamamos ‚Äúdemostraciones‚Äù) para determinar qu√© funci√≥n de costos deber√≠a optimizar?</p> <p>Si la persona conduce a la defensiva, la funci√≥n de costes que explicar√° sus acciones pondr√° mucho peso en la seguridad y menos en la eficiencia. El robot puede adoptar esta funci√≥n de costes como propia y optimizarla cuando conduce el propio coche.</p> <p>Hay otras formas en que una persona puede dar su opini√≥n. Una persona podr√≠a utilizar el lenguaje en lugar de la demostraci√≥n para instruir al robot. Una persona podr√≠a actuar como cr√≠tico, observando al robot realizar una tarea de una manera (o dos) y luego diciendo qu√© tan bien se hizo la tarea (o de qu√© manera fue mejor), o dando consejos sobre c√≥mo mejorar.</p> <h4 id="politicas-de-aprendizaje-directamente-a-traves-de-la-imitacion">Pol√≠ticas de aprendizaje directamente a trav√©s de la imitaci√≥n<a class="headerlink" href="#politicas-de-aprendizaje-directamente-a-traves-de-la-imitacion" title="Permanent link">¬∂</a></h4> <p>Una alternativa es evitar las funciones de costos y aprender directamente la pol√≠tica de robot deseada. En nuestro ejemplo de autom√≥vil, las demostraciones humanas generan un conveniente conjunto de datos de estados etiquetados por la acci√≥n que el robot debe realizar en cada estado. El robot puede ejecutar aprendizaje supervisado para ajustarse a una pol√≠tica, y ejecutar esa pol√≠tica. Esto se llama <strong>aprendizaje por imitaci√≥n</strong> o <strong>clonaci√≥n conductual</strong>.</p> <p>Un desaf√≠o con este enfoque es la generalizaci√≥n a nuevos estados. El robot no sabe por qu√© las acciones en su base de datos han sido marcadas como √≥ptimas. No tiene regla causal; todo lo que puede hacer es ejecutar un algoritmo de aprendizaje supervisado para intentar aprender una pol√≠tica que se generalizar√° a estados desconocidos. Sin embargo, no hay garant√≠a de que la generalizaci√≥n sea correcta.</p> <p>El robot puede ajustarse a un modelo din√°mico basado en las demostraciones y luego utilizar un control √≥ptimo para generar una pol√≠tica que optimice la permanencia cerca de la demostraci√≥n. Se ha utilizado una versi√≥n de esto para realizar maniobras muy desafiantes a nivel experto en un peque√±o helic√≥ptero radiocontrolado.</p> <p>T√©cnicas recientes relacionadas utilizan entrenamiento adversario: alternan entre entrenar a un clasificador para distinguir entre la pol√≠tica aprendida del robot y las demostraciones del humano, y entrenar una nueva pol√≠tica del robot mediante aprendizaje reforzado para enga√±ar al clasificador. Estos avances permiten al robot manejar estados que est√°n cerca de las demostraciones, pero la generalizaci√≥n a estados lejanos o a nuevas din√°micas es un trabajo en progreso.</p> <h2 id="dominios-de-aplicacion">Dominios de aplicaci√≥n<a class="headerlink" href="#dominios-de-aplicacion" title="Permanent link">¬∂</a></h2> <p>La tecnolog√≠a rob√≥tica ya est√° impregnando nuestro mundo y tiene el potencial de mejorar nuestra independencia, salud y productividad. A continuaci√≥n se muestran algunas aplicaciones de ejemplo.</p> <p><strong>Cuidados en el hogar:</strong> Los robots han comenzado a ingresar a los hogares para cuidar a los adultos mayores y a las personas con discapacidad motriz, ayud√°ndolos con las actividades de la vida diaria y permiti√©ndoles vivir de manera m√°s independiente. Estos incluyen sillas de ruedas y brazos montados en sillas de ruedas como el brazo Kinova. Aunque al principio son operados directamente por un humano, estos robots est√°n ganando cada vez m√°s autonom√≠a. En el horizonte hay robots operados mediante interfaces cerebro-m√°quina, que se ha demostrado que permiten a las personas con cuadriplej√≠a utilizar un brazo rob√≥tico para agarrar objetos e incluso alimentarse. Relacionados con esto est√°n las pr√≥tesis que responden inteligentemente a nuestras acciones y los exoesqueletos que nos dan una fuerza sobrehumana o permiten que las personas que no pueden controlar sus m√∫sculos de la cintura para abajo vuelvan a caminar.</p> <p><img alt="Brazo Kinova" src="assets/u=https%3A%2F%2Ftse4.mm.bing.net%2Fth%3Fid%3DOIP.jpeg"/></p> <p>Los robots personales est√°n destinados a ayudarnos con tareas diarias como limpiar y organizar, liber√°ndonos tiempo. Aunque la manipulaci√≥n todav√≠a tiene un camino por recorrer antes de que pueda funcionar sin problemas en entornos humanos desordenados y desestructurados, la navegaci√≥n ha logrado algunos avances. En particular, muchos hogares ya cuentan con un robot aspirador m√≥vil.</p> <p><img alt="JASPER, robots aspiradores b√°sicos - Robots al Detalle" src="assets/u=https%3A%2F%2Frobotsaldetalle.es%2Fwp-content%2Fuploads%2F2011%2F05%2FJASPEr-robot-aspirador-modelos.jpeg"/></p> <p><strong>Atenci√≥n m√©dica:</strong> los robots ayudan y potencian a los cirujanos, permitiendo procedimientos m√°s precisos, m√≠nimamente invasivos y seguros con mejores resultados para los pacientes. El robot quir√∫rgico Da Vinci ahora se utiliza ampliamente en hospitales de EE. UU.</p> <p><img alt="Robot Da Vinci para cirug√≠a rob√≥tica. Especialidades y usos" src="assets/u=https%3A%2F%2Frevistaderobots.com%2Fwp-content%2Fuploads%2F2020%2F07%2FRobot-Da-Vinci-para-pr%C3%B3stata-y-precio-en-la-seguridad-social.jpeg"/></p> <p><strong>Servicios</strong>: Los robots m√≥viles ayudan en edificios de oficinas, hoteles y hospitales. Savioke ha instalado robots en hoteles que entregan productos como toallas o pasta de dientes en la habitaci√≥n. Los robots Helpmate y TUG transportan alimentos y medicamentos en los hospitales, mientras que el robot Moxi de Diligent Robotics ayuda a las enfermeras con las responsabilidades log√≠sticas de back-end. Co-Bot deambula por los pasillos de la Universidad Carnegie Mellon, listo para guiarte a la oficina de alguien. Tambi√©n podemos utilizar robots de telepresencia como el Beam para asistir a reuniones y conferencias de forma remota, robots de Telepresencia o controlar a nuestros abuelos.</p> <p><strong>Autom√≥viles aut√≥nomos:</strong> algunos de nosotros ocasionalmente nos distraemos mientras conducimos, con llamadas de tel√©fono celular, mensajes de texto u otras distracciones. El triste resultado: m√°s de un mill√≥n de personas mueren cada a√±o en accidentes de tr√°fico. Adem√°s, muchos de nosotros pasamos mucho tiempo conduciendo y nos gustar√≠a recuperar parte de ese tiempo. Todo esto ha llevado a un esfuerzo masivo y continuo para implementar autom√≥viles aut√≥nomos.</p> <p><strong>Entretenimiento</strong>: Disney ha estado utilizando robots (bajo el nombre de animatronics) en sus parques desde 1963. Originalmente, estos robots estaban restringidos a movimientos (y habla) invariables, de circuito abierto y dise√±ados a mano, pero desde 2009 una versi√≥n llamada autonomatronics puede generar acciones aut√≥nomas. Los robots tambi√©n toman la forma de juguetes inteligentes para ni√±os; por ejemplo, Cozmo de Anki juega con ni√±os y puede golpear la mesa con frustraci√≥n cuando pierde. Finalmente, los cuadrotores como el R1 de Skydio de la act√∫an como fot√≥grafos y camar√≥grafos personales, sigui√©ndonos para tomar fotograf√≠as de acci√≥n mientras esquiamos o andamos en bicicleta.</p> <p><img alt="Skydio R1 drone tracks and videos your ride - even through the trees" src="assets/u=http%3A%2F%2Fcontent.bikeroar.com%2Fsystem%2Fcontent%2F000%2F423%2F298%2Foriginal%2FSkydio_R1_Self-Flying_Camera_for_cyclists.jpeg" style="zoom:25%;"/></p> <p><strong>Exploraci√≥n y entornos peligrosos:</strong> los robots han llegado a lugares donde ning√∫n ser humano hab√≠a llegado antes, incluida la superficie de Marte. Los brazos rob√≥ticos ayudan a los astronautas a desplegar y recuperar sat√©lites y a construir la Estaci√≥n Espacial Internacional. Los robots tambi√©n ayudan a explorar bajo el mar. Se utilizan habitualmente para adquirir mapas de barcos hundidos. En 1996, un equipo de investigadores introdujo un robot con patas en el cr√°ter de un volc√°n activo para adquirir datos para la investigaci√≥n clim√°tica. Los robots se est√°n convirtiendo en herramientas muy efectivas para recopilar informaci√≥n en dominios de dif√≠cil (o peligroso) acceso para las personas.</p> <p>Los robots han ayudado a las personas a limpiar desechos nucleares, sobre todo en Three Mile Island, Chernobyl y Fukushima. Los robots estuvieron presentes despu√©s del colapso del World Trade Center, donde ingresaron a estructuras consideradas demasiado peligrosas para los equipos humanos de b√∫squeda y rescate. Tambi√©n en este caso, estos robots se implementan inicialmente mediante teleoperaci√≥n y, a medida que avanza la tecnolog√≠a, se vuelven cada vez m√°s aut√≥nomos, con un operador humano a cargo pero sin tener que especificar cada comando.</p> <p><strong>Industria</strong>: la mayor√≠a de los robots actuales se implementan en f√°bricas, automatizando tareas que son dif√≠ciles, peligrosas o aburridas para los humanos. (La mayor√≠a de los robots industriales se encuentran en f√°bricas de autom√≥viles). Automatizar estas tareas es positivo en t√©rminos de producir eficientemente lo que la sociedad necesita. Al mismo tiempo, tambi√©n significa desplazar a algunos trabajadores humanos de sus puestos de trabajo. Esto tiene importantes implicaciones pol√≠ticas y econ√≥micas: la necesidad de reentrenamiento y educaci√≥n, la necesidad de una divisi√≥n justa de los recursos, etc.</p> <h2 id="resumen">Resumen<a class="headerlink" href="#resumen" title="Permanent link">¬∂</a></h2> <p>La rob√≥tica se trata de agentes encarnados f√≠sicamente, que pueden cambiar el estado del mundo f√≠sico. En este cap√≠tulo, hemos aprendido lo siguiente:</p> <ul> <li>Los tipos de robots m√°s comunes son los manipuladores (brazos rob√≥ticos) y los robots m√≥viles. Tienen sensores para percibir el mundo y actuadores que producen movimiento, que luego afecta al mundo a trav√©s de efectores.</li> <li>El problema general de la rob√≥tica implica la estocasticidad (que puede ser manejada por los MDP), la observabilidad parcial (que puede ser manejada por los POMDP) ‚Äã‚Äãy la actuaci√≥n con y alrededor de otros agentes (que puede ser manejada con la teor√≠a de juegos). El problema se complica a√∫n m√°s por el hecho de que la mayor√≠a de los robots trabajan en espacios de acci√≥n y estados continuos y de alta dimensi√≥n. Tambi√©n operan en el mundo real, que se niega a correr m√°s r√°pido que el tiempo real y en el que los fallos provocan da√±os a cosas reales, sin posibilidad de "deshacerlas".</li> <li>Idealmente, el robot resolver√≠a todo el problema de una vez: las observaciones en forma de se√±ales brutas de los sensores entran y las acciones en forma de pares o corrientes a los motores salen. Sin embargo, en la pr√°ctica esto resulta demasiado desalentador y los rob√≥ticos suelen desacoplar diferentes aspectos del problema y tratarlos de forma independiente.</li> <li>Normalmente separamos la percepci√≥n (estimaci√≥n) de la acci√≥n (generaci√≥n de movimiento). La percepci√≥n en rob√≥tica implica visi√≥n por computadora para reconocer el entorno a trav√©s de c√°maras, pero tambi√©n localizaci√≥n y mapeo.</li> <li>La percepci√≥n rob√≥tica se ocupa de estimar cantidades relevantes para la decisi√≥n a partir de datos de sensores. Para hacerlo, necesitamos una representaci√≥n interna y un m√©todo para actualizar esta representaci√≥n interna a lo largo del tiempo.</li> <li>Los algoritmos de filtrado probabil√≠stico, como los filtros de part√≠culas y los filtros de Kalman, son √∫tiles para la percepci√≥n de los robots. Estas t√©cnicas mantienen el estado de creencia, una distribuci√≥n posterior sobre las variables de estado.</li> <li>Para generar movimiento, utilizamos espacios de configuraci√≥n, donde un punto especifica todo lo que necesitamos saber para ubicar cada punto del cuerpo del robot. Por ejemplo, para un brazo rob√≥tico con dos articulaciones, una configuraci√≥n consta de dos √°ngulos de articulaci√≥n.</li> <li>Normalmente desacoplamos el problema de generaci√≥n de movimiento en planificaci√≥n de movimiento, que se ocupa de producir un plan, y control de seguimiento de trayectoria, que se ocupa de producir una pol√≠tica para las entradas de control (comandos de actuador) que resulta en la ejecuci√≥n del plan.</li> <li>La planificaci√≥n del movimiento se puede resolver mediante la b√∫squeda de gr√°ficos mediante descomposici√≥n celular; utilizar algoritmos de planificaci√≥n de movimiento aleatorios, que muestrean hitos en el espacio de configuraci√≥n continua; o utilizar la optimizaci√≥n de trayectoria, que puede hacer que una trayectoria en l√≠nea recta evite la colisi√≥n de forma iterativa aprovechando un campo de distancia con signo.</li> <li>Una ruta encontrada mediante un algoritmo de b√∫squeda se puede ejecutar utilizando la ruta como trayectoria de referencia para un controlador PID, que corrige constantemente los errores entre el lugar donde est√° el robot y donde se supone que debe estar, o mediante un control de par calculado, que agrega un T√©rmino de avance que hace uso de la din√°mica inversa para calcular aproximadamente qu√© par enviar para avanzar a lo largo de la trayectoria.</li> <li>El control √≥ptimo une la planificaci√≥n del movimiento y el seguimiento de la trayectoria calculando una trayectoria √≥ptima directamente sobre las entradas de control. Esto es especialmente f√°cil cuando tenemos costos cuadr√°ticos y din√°mica lineal, lo que da como resultado un regulador cuadr√°tico lineal (LQR). Los m√©todos populares hacen uso de esto linealizando la din√°mica y calculando aproximaciones del costo de segundo orden (ILQR).</li> <li>La planificaci√≥n bajo incertidumbre une la percepci√≥n y la acci√≥n mediante la replanificaci√≥n en l√≠nea (como el control predictivo de modelos) y acciones de recopilaci√≥n de informaci√≥n que ayudan a la percepci√≥n.</li> <li>El aprendizaje por refuerzo se aplica en rob√≥tica, con t√©cnicas que intentan reducir el n√∫mero requerido de interacciones con el mundo real. Estas t√©cnicas tienden a explotar los modelos, ya sea estim√°ndolos y utiliz√°ndolos para planificar, o formando pol√≠ticas que sean s√≥lidas con respecto a diferentes par√°metros posibles del modelo.</li> <li>La interacci√≥n con los humanos requiere la capacidad de coordinar las acciones del robot con las de ellos, lo que puede formularse como un juego. Generalmente descomponemos la soluci√≥n en predicci√≥n, en la que utilizamos las acciones en curso de la persona para estimar lo que har√° en el futuro, y acci√≥n, en la que utilizamos las predicciones para calcular el movimiento √≥ptimo para el robot.</li> <li>Ayudar a los humanos tambi√©n requiere la capacidad de aprender o inferir lo que quieren. Los robots pueden abordar esto aprendiendo la funci√≥n de costo deseada que deben optimizar a partir del aporte humano, como demostraciones, correcciones o instrucci√≥n en lenguaje natural. Alternativamente, los robots pueden imitar el comportamiento humano y utilizar el aprendizaje por refuerzo para ayudar a afrontar el desaf√≠o de la generalizaci√≥n a nuevos estados.</li> </ul></div> <aside class="md-source-file"> <span class="md-source-file__fact"> <span class="md-icon" title="√öltima actualizaci√≥n"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M21 13.1c-.1 0-.3.1-.4.2l-1 1 2.1 2.1 1-1c.2-.2.2-.6 0-.8l-1.3-1.3c-.1-.1-.2-.2-.4-.2m-1.9 1.8-6.1 6V23h2.1l6.1-6.1zM12.5 7v5.2l4 2.4-1 1L11 13V7zM11 21.9c-5.1-.5-9-4.8-9-9.9C2 6.5 6.5 2 12 2c5.3 0 9.6 4.1 10 9.3-.3-.1-.6-.2-1-.2s-.7.1-1 .2C19.6 7.2 16.2 4 12 4c-4.4 0-8 3.6-8 8 0 4.1 3.1 7.5 7.1 7.9l-.1.2z"></path></svg> </span> <span class="git-revision-date-localized-plugin git-revision-date-localized-plugin-date" title="15 de julio de 2025 18:19:31 UTC">15 de julio de 2025</span> </span> </aside> </article> </div> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button class="md-top md-icon" data-md-component="top" hidden="" type="button"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Volver al principio </button> </main> <footer class="md-footer"> <nav aria-label="Pie" class="md-footer__inner md-grid"> <a aria-label="Anterior: Actividades Entregables" class="md-footer__link md-footer__link--prev" href="../UD03/UD03_ActividadesEntregables.html"> <div class="md-footer__button md-icon"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </div> <div class="md-footer__title"> <span class="md-footer__direction"> Anterior </span> <div class="md-ellipsis"> Actividades Entregables </div> </div> </a> <a aria-label="Siguiente: Actividades Guiadas" class="md-footer__link md-footer__link--next" href="UD04_ActividadesGuiadas.html"> <div class="md-footer__title"> <span class="md-footer__direction"> Siguiente </span> <div class="md-ellipsis"> Actividades Guiadas </div> </div> <div class="md-footer__button md-icon"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11z"></path></svg> </div> </a> </nav> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class="md-copyright"> <div class="md-copyright__highlight"> ¬© 2025 David Mart√≠nez licensed under CC BY-NC-SA 4.0 </div> Made with <a href="https://squidfunk.github.io/mkdocs-material/" rel="noopener" target="_blank"> Material for MkDocs </a> </div> <div class="md-social"> <a class="md-social__link" href="https://youtube.com/@martinezpenya" rel="noopener" target="_blank" title="youtube.com"> <svg viewbox="0 0 24 24" xmlns="http://www.w3.org/2000/svg"><path d="M23.498 6.186a3.02 3.02 0 0 0-2.122-2.136C19.505 3.545 12 3.545 12 3.545s-7.505 0-9.377.505A3.02 3.02 0 0 0 .502 6.186C0 8.07 0 12 0 12s0 3.93.502 5.814a3.02 3.02 0 0 0 2.122 2.136c1.871.505 9.376.505 9.376.505s7.505 0 9.377-.505a3.02 3.02 0 0 0 2.122-2.136C24 15.93 24 12 24 12s0-3.93-.502-5.814M9.545 15.568V8.432L15.818 12z"></path></svg> </a> <a class="md-social__link" href="https://linkedin.com/in/martinezpenya" rel="noopener" target="_blank" title="linkedin.com"> <svg viewbox="0 0 448 512" xmlns="http://www.w3.org/2000/svg"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M416 32H31.9C14.3 32 0 46.5 0 64.3v383.4C0 465.5 14.3 480 31.9 480H416c17.6 0 32-14.5 32-32.3V64.3c0-17.8-14.4-32.3-32-32.3M135.4 416H69V202.2h66.5V416zM102.2 96a38.5 38.5 0 1 1 0 77 38.5 38.5 0 1 1 0-77m282.1 320h-66.4V312c0-24.8-.5-56.7-34.5-56.7-34.6 0-39.9 27-39.9 54.9V416h-66.4V202.2h63.7v29.2h.9c8.9-16.8 30.6-34.5 62.9-34.5 67.2 0 79.7 44.3 79.7 101.9z"></path></svg> </a> <a class="md-social__link" href="https://github.com/martinezpenya" rel="noopener" target="_blank" title="github.com"> <svg viewbox="0 0 512 512" xmlns="http://www.w3.org/2000/svg"><!-- Font Awesome Free 7.0.1 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </a> </div> </div> </div> </footer> </div> <div class="md-dialog" data-md-component="dialog"> <div class="md-dialog__inner md-typeset"></div> </div> <script id="__config" type="application/json">{"base": "..", "features": ["header.autohide", "content.action.edit", "content.code.copy", "content.code.select", "content.code.anotate", "toc.follow", "toc.integrate", "navigation.top", "navigation.footer"], "search": "../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copiado al portapapeles", "clipboard.copy": "Copiar al portapapeles", "search.result.more.one": "1 m\u00e1s en esta p\u00e1gina", "search.result.more.other": "# m\u00e1s en esta p\u00e1gina", "search.result.none": "No se encontraron documentos", "search.result.one": "1 documento encontrado", "search.result.other": "# documentos encontrados", "search.result.placeholder": "Teclee para comenzar b\u00fasqueda", "search.result.term.missing": "Falta", "select.version": "Seleccionar versi\u00f3n"}, "version": null}</script> <script src="../assets/javascripts/bundle.f55a23d4.min.js"></script> <script src="../js/mathjax-config.js"></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js"></script> </body> </html>